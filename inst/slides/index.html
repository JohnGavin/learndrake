<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Reproducible workflows at scale with drake</title>
    <meta charset="utf-8" />
    <meta name="author" content="Will Landau" />
    <link href="index_files/remark-css/default.css" rel="stylesheet" />
    <link href="index_files/remark-css/default-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Reproducible workflows at scale with drake
### Will Landau

---


&lt;style&gt;
.inverse {
background-color: transparent;
text-shadow: 0 0 0px transparent;
}
.title-slide {
vertical-align: bottom !important; 
text-align: center !important;
}
.title-slide h1 {
position: absolute;
top: 0;
left: 0;
right: 0;
width: 100%;
line-height: 4em;
color: #666666;
}
.title-slide h3 {
line-height: 6em;
color: #666666;
}
.title-slide {
background-color: white;
background-image: url('images/logo.png');
background-repeat: no-repeat;
background-size: 25%;
}
.remark-slide-content:after {
content: "Copyright Eli Lilly and Company";
position: absolute;
bottom: -5px;
left: 20px;
height: 40px;
width: 100%;
font-family: Helvetica, Arial, sans-serif;
font-size: 0.7em;
color: gray;
background-repeat: no-repeat;
background-size: contain;
}
&lt;/style&gt;



## Large data science workflows

- Struggles

  1. Long runtimes.
  2. Many tasks.
  3. Interconnected tasks.

- Examples
  - Deep learning.
  - Classical machine learning.
  - Bayesian data analysis via Markov chain Monte Carlo.
  - Spatial data analysis.
  - Clinical trial modeling and simulation.
  - Subgroup identification.
  - Graph-based multiple comparison procedures.
  - Genomics pipelines.
  - PK/PD modeling.

???

Let's talk about what happens in large data science projects. Projects that are ambitious and complicated, that have a lot of moving parts and long runtimes. Projects that are difficult just to wrap your head around, that are difficult just to run from start to finish, but whose results really matter and carry important consequences.

---

## Interconnected tasks
&lt;center&gt;
&lt;img src = "./images/workflow.png" align="middle" style="border: none; box-shadow: none; text-align: center;"&gt;
&lt;/center&gt;

???

The tasks of a data analysis project are interconnected.

---

## When you change something...
&lt;center&gt;
&lt;img src = "./images/change.png" align="middle" style="border: none; box-shadow: none; text-align: center;"&gt;
&lt;/center&gt;

???

When you go back and change something...

---

## ...the downstream output is **no longer valid**.

&lt;center&gt;
&lt;img src = "./images/downstream.png" align="middle" style="border: none; box-shadow: none; text-align: center;"&gt;
&lt;/center&gt;

???

...everything that depends on it is no longer valid. And somehow you gotta pick up the pieces.

---

## Do you rerun **everything** from scratch?

- Not if you deal with long runtimes!

&lt;center&gt;
&lt;img src = "./images/sisyphus.svg" align="middle" style="border: none; box-shadow: none; height: 375px; text-align: center;"&gt;
&lt;div style="font-size: 0.5em; text-align: center"&gt;&lt;a href="https://openclipart.org/detail/275842/sisyphus-overcoming-silhouette"&gt;https://openclipart.org/detail/275842/sisyphus-overcoming-silhouette&lt;/a&gt;&lt;/div&gt;
&lt;/center&gt;

???

For large projects, this is where the programming and workflow management techniques we are used to start to break down. If it takes several hours just to fit a single model, you're not going to want to rerun your whole analysis every time you make a change. It's just too much frustration.

But I used to do this all the time. In my dissertation work, every model took about 4 hours, and I needed results from about 20 or 30 of them. I spent the last 6 months of grad school mostly restarting code and waiting it to finish. My results were out of sync, and it was hard to get any writing done.

---

## Do you pick and choose what to update?

- Messy.
- Prone to human error.
- Not reproducible.

&lt;center&gt;
&lt;img src = "./images/mess.svg" align="middle" style="border: none; box-shadow: none; height: 400px; text-align: center;"&gt;
&lt;div style="font-size: 0.5em; text-align: center;"&gt;&lt;a href="https://openclipart.org/detail/216179/messy-desk"&gt;https://openclipart.org/detail/216179/messy-desk&lt;/a&gt;&lt;/div&gt;
&lt;/center&gt;

???

But it's perilous to try to stop this cycle on your own without any help. If I'm the one deciding which code to skip and which code to rerun, it's a mess. It's too easy to make big mistakes, and the results are not reproducible. I would not trust a human to do this.

---

## Solution: pipeline tools

&lt;center&gt;
&lt;img src = "./images/infographic.svg" align="middle" style="border: none; box-shadow: none; text-align: center;"&gt;
&lt;/center&gt;

- Tons exist already: [github.com/pditommaso/awesome-pipeline](https://github.com/pditommaso/awesome-pipeline).
- Most are language-agnostic or designed for Python or the shell.

???

Fortunately, we can *automate* the process of bringing results up to date as fast as possible. The standard solution is a pipeline tool. You may have heard of examples already. There's

- Make
- Snakemake
- Airflow
- Nextflow
- Luigi
- Dask

But they're difficult to use with R. Most of them try to be language-agnostic for the sake of versatility, but this means they add extra friction when you try to work with any one language in particular. My experience with Make specifically is that it obstructs my relationship with R. It pulls me out of the language and into sub-optimal programming practices.

---

## What distinguishes `drake`?

&lt;center&gt;
&lt;img src = "./images/R.png" align="middle" style="border: none; box-shadow: none; text-align: center; height: 200px"&gt;
&lt;/center&gt;

- Aggressively designed for R.
1. Think **functions**, not script files.
2. Think **variables**, not output files.
3. Think **data frames**, not `Makefile`s.
- [`drake`](https://github.com/ropensci/drake) borrows (1) and (2) from the [`remake`](https://github.com/richfitz/remake) package by [Rich FitzJohn](https://github.com/richfitz).
- [`remake`](https://github.com/richfitz/remake) is no longer under development.
- [`drake`](https://github.com/ropensci/drake) tries to extend [`remake`](https://github.com/richfitz/remake)'s ideas further and handle larger projects.

???


drake, on the other hand, is designed for R at its very core. It works entirely *within* the language, and it nudges you to write good code.

1. Instead of imperative script files, you write **functions**.

2. Instead of saving output files manually and dealing with all the headache of trying to organize them yourself, you return **objects** from those functions, and drake saves them for you automatically.

3. And instead of Makefiles in Make or YAML files in remake, you have an R-focused interface to define the skippable steps of your workflow.

By the way, drake was not the first pipeline tool to respect the way R works. Before drake, there was a package called `remake` by Rich FitzJohn. remake was playing nicely with objects and functions long before I created drake.

But remake is no longer maintained. And because it required you to work with YAML configuration files and supply your actual R scripts directly, it kind of had one foot in and one foot out anyway. drake tries to finish what remake started and scale up the intensity of work you can do with it.

---

## Example: a deep learning workflow

- Goal: predict customers who cancel their subscriptions with a telecom company.
- Data: [IBM Watson Telco Customer Churn dataset](https://www.ibm.com/communities/analytics/watson-analytics-blog/predictive-insights-in-the-telco-customer-churn-data-set/).
- Workflow principles generalize to other industries.

&lt;img src = "./images/combine.png" style="border: none; box-shadow: none; height: 200px"&gt;

&lt;div style="font-size: 0.5em;"&gt;&lt;a href="https://openclipart.org/detail/90739/newplus"&gt;https://openclipart.org/detail/90739/newplus&lt;/a&gt;, &lt;a href="https://github.com/rstudio/keras"&gt;https://github.com/rstudio/keras&lt;/a&gt;&lt;/div&gt;

???

To dive into drake, we're going to use a machine learning example. We've got a deep neural net, and we're going to use it to predict "customer churn", which is another way of saying attrition, or dropout.

---

background-image: ./images/not.png

## &lt;img src="./images/no.png" width="40" height="40"&gt; Let's move beyond numbered scripts.


```r
run_everything.R
R/
├── 01-data.R
├── 02-munge.R
├── 03-model.R
├── 04-results.R
└── 05-plot.R
data/
└── customer_churn.csv
output/
├── model_relu.h5
├── model_sigmoid.h5
├── confusion_matrix.rds
└── metrics_plot.png
```

???

As soon as we start to set up a project like this, we often find that it's too big for the classic "numbered script" paradigm you may have seen. It falls apart pretty easily.

---

## &lt;img src="./images/no.png" width="40" height="40"&gt; Why not numbered scripts?

- The planning and the execution happen at the same time.
- Too cumbersome, ad hoc, and tangled for ambitious projects.


```r
# 02-munge.R
*library(recipes) # Package dependencies scattered across scripts.

*rec &lt;- data %&gt;% # Single-use code, difficult to test.
  training() %&gt;%
  recipe(Churn ~ .) %&gt;%
  step_rm(customerID) %&gt;%
  step_naomit(all_outcomes(), all_predictors()) %&gt;%
  step_discretize(tenure, options = list(cuts = 6)) %&gt;%
  step_log(TotalCharges) %&gt;%
  step_mutate(Churn = ifelse(Churn == "Yes", 1, 0)) %&gt;%
  step_dummy(all_nominal(), -all_outcomes()) %&gt;%
  step_center(all_predictors(), -all_outcomes()) %&gt;%
  step_scale(all_predictors(), -all_outcomes()) %&gt;%
  prep()

*saveRDS(rec, "recipe.rds") # Final output scattered across code.
```

???

And the reason is that it tries to do too many things at once. The planning and the execution happen at the same time, and the actual content of the analysis is muddled with the bookkeeping.

In these scripts, you've got package dependencies and file-saving steps scattered all over the place. And for this preprocessing recipe, it looks like one thing, but it's really two things. You're defining a complicated procedure, and you're running it **as** you're defining it. These things should really be separated. Otherwise, it's hard to take the code out of context and test it.

But more importantly, we don't have shorthand names for the long complicated steps we're doing. We don't have the structure to organize the ideas, so it's hard to take a step back and think about what we're doing.

In the R community, we like to criticize Microsoft Excel. We give it a hard time for putting the formulas and the spreadsheets in the same file, and we claim it's not reproducible. And we have a point. But we are guilty of something very similar! We have a habit of knitting and weaving and tangling too many things together that don't belong together!

---

## &lt;img src="./images/yes.png" width="60" height="40"&gt; Instead, embrace **functions**!

- A function is a reusable command that accepts one or more inputs and returns a single output.


```r
my_function &lt;- function(argument1, argument2) {
  argument1 + argument2
}

my_function(1, 2)
## [1] 3

my_function(3, 4)
## [1] 7
```

???

Arguably the easiest, most ubiquitous, most standard way to solve this problem is to write functions.

A function is just a reusable command that accepts one or more inputs and returns a single output. You define it once, you give it a name, and you call it whenever you need it on whatever data that fits.

---

## Why use functions?

1. Clarity: break down complicated ideas into manageable pieces.
2. Use R as intended.

    &gt;    - Everything that exists is an object.
    &gt;    - Everything that happens is a function call.
    &gt;
    &gt; John Chambers

3. Reuse: define once, run wherever.

???

But functions do so much more than that. They're not just for code you want to repeat and reuse. They're for code you want to **understand**.

Functions break down complicated ideas into manageable pieces. Instead of a huge mess, you deal with pieces that are smaller, cleaner, less intimidating, and that mean something clear and useful.

Functions are like paragraphs. It's the same mental experience inside your head. You wouldn't write an endless wall of text as a paper or an email, right? It's the same thing here, and R Markdown chunks don't have the structure to get us there. With functions, you're crystallizing a stream of consciousness into clear, discrete, explainable, composable, chainable points you want to make.

Now this may seem like a lot of extra work, but writing any kind of code is always a challenge. It always going to take focus and effort. And the more we accept that and lean into it, the better off we are in the long run, and the more things make sense. 

For example, you'll notice that you're being truer to the process of data analysis itself. Functions are **fundamentally designed** to describe change. And in our workflows, we change raw data into clean data, clean data into model output, and so on. Those steps are *begging* to be functions.

You're also being true to the R language. Much of R is built on functional programming, so you're using it how it's **designed** to be used.

---

## Functions in a workflow


```r
make.R
R/
├── packages.R
*├── functions.R
└── plan.R
data/
└── customer_churn.csv
.drake/ # drake's cache
└──     # Output automatically appears here.
```

???

So let's use functions in the deep learning example. Functions are going to occupy most of the real estate in our scripts.

---

## Functions in a workflow


```r
*# packages.R: all package dependencies
library(recipes)
# other packages...
```


```r
*# functions.R: pure reusable code
prepare_recipe &lt;- function(data) {
  data %&gt;%
    training() %&gt;%
    recipe(Churn ~ .) %&gt;%
    step_rm(customerID) %&gt;%
    step_naomit(all_outcomes(), all_predictors()) %&gt;%
    step_discretize(tenure, options = list(cuts = 6)) %&gt;%
    step_log(TotalCharges) %&gt;%
    step_mutate(Churn = ifelse(Churn == "Yes", 1, 0)) %&gt;%
    step_dummy(all_nominal(), -all_outcomes()) %&gt;%
    step_center(all_predictors(), -all_outcomes()) %&gt;%
    step_scale(all_predictors(), -all_outcomes()) %&gt;%
    prep()
}
# other functions...
```

???

Our preprocessing task is a single, clear, distinct idea with clearly-defined inputs and a single return value. So we put it in a function.

---

## Functions in a workflow


```r
# later in functions.R...

run_everything &lt;- function() {
  data &lt;- read_csv(file_in("data/customer_churn.csv"), col_types = cols()) %&gt;%
    initial_split(prop = 0.3)
  saveRDS(data, "output/data.rds")

* rec &lt;- prepare_recipe(data) # Call your other functions.
  saveRDS(rec, "output/rec.rds")

  model_relu &lt;- train_model(rec, act1 = "relu")
  save_model_hdf5(model_relu, "output/model_relu.h5")
  # more models...

  conf_sigmoid &lt;- confusion_matrix(data, rec, model_sigmoid)
  saveRDS(conf_sigmoid, "output/conf_sigmoid.rds")
  # more confusion matrices...

  metrics &lt;- compare_models(conf_relu, conf_sigmoid)
  saveRDS(metrics, "output/metrics.rds")
}
```

???

And we can take the functions we define and call them from other functions, including a function that runs everything at the top level and controls the overall flow of the analysis.

---

## Conduct your analysis with your **functions**.


```r
# run_everything.R
source("R/packages.R")
source("R/functions.R")
run_everything()
```

???

Our top-level *script* pretty much just calls our flagship function at the top. And that's a huge improvement.

---

## But we can still do better...

- Avoid rerunning all the computation every time.
- Avoid micromanaging output files.

&lt;center&gt;
&lt;img src = "./images/genie.png" align="middle" style="border: none; box-shadow: none; height: 375px; text-align: center;"&gt;
&lt;div style="font-size: 0.5em; text-align: center"&gt;&lt;a href="https://publicdomainvectors.org/en/free-clipart/Golden-magic-lamp/61683.html"&gt;https://publicdomainvectors.org/en/free-clipart/Golden-magic-lamp/61683.html&lt;/a&gt;&lt;/div&gt;
&lt;/center&gt;

???

But we can still do better. We can avoid rerunning all the computation every time. We can avoid micromanaging output files. All we need to do is cash in on all that hard work we spent writing functions to use `drake`.

---

## Enter drake! Define a **plan**.


```r
plan &lt;- drake_plan(
* rec = prepare_recipe(data), # Use your functions.
  model = target(
    train_model(rec, act1 = act),
    format = "keras",
    transform = map(act = c("relu", "sigmoid"))
  ),
  conf = target(
    confusion_matrix(data, rec, model),
    transform = map(model, .id = act)
  ),
  metrics = target(
    compare_models(conf),
    transform = combine(conf)
  ),
* data = read_csv(                      # flexible target order,
*   file_in("data/customer_churn.csv"), # flexible commands
    col_types = cols()
  ) %&gt;%
    initial_split(prop = 0.3)
)
```

???

Instead of a `run_everything()` function, we create a drake plan. The plan defines a bunch of skippable steps. Each step has a command, and each command calls the R functions it needs to and returns a value called a **target**. We give these targets names on the left, and we list them in any order we want. So we can even define the data at the bottom here.

---

## The plan is a data frame of skippable tasks.


```r
plan
## # A tibble: 7 x 3
##   target      command                                                format
##   &lt;chr&gt;       &lt;expr&gt;                                                 &lt;chr&gt; 
## 1 rec         prepare_recipe(data)                                 … &lt;NA&gt;  
## 2 model_relu  train_model(rec, act1 = "relu")                      … keras 
## 3 model_sigm… train_model(rec, act1 = "sigmoid")                   … keras 
## 4 conf_relu   confusion_matrix(data, rec, model_relu)              … &lt;NA&gt;  
## 5 conf_sigmo… confusion_matrix(data, rec, model_sigmoid)           … &lt;NA&gt;  
## 6 metrics     compare_models(conf_relu, conf_sigmoid)              … &lt;NA&gt;  
## 7 data        read_csv(file_in("data/customer_churn.csv"), col_type… &lt;NA&gt;
```

???

This plan is a tidy data frame of commands and targets. This is what you get instead of a Makefile. It exists entirely within R, and it uses a data structure we all like.

---

## The workflow

&lt;br&gt;
&lt;center&gt;
&lt;img align="middle" src = "./images/vis1.png" style="border: none; box-shadow: none;"&gt;
&lt;/center&gt;

???

Now as a first step, `drake` takes all this in, and it analyzes your plan, and it analyzes your functions, without running any of your code just yet. It detects the symbols that you mention, and any optional input files you choose to declare, and it decides which steps go first and which steps go last. So it puts "rec" after "data" because the command for "rec" mentions the symbol "data". And similarly, it resolves an entire flowchart of tasks.

---

## Run the project in make.R.


```r
# like run_everything.R...
source("R/packages.R")
source("R/functions.R")
source("R/plan.R")

make(plan)
## target data
## target rec
## target model_relu
## target model_sigmoid
## target conf_relu
## target conf_sigmoid
## target metrics
```

???

So after you source your packages and functions into your session, the `make()` function does all that setup and code analysis I described, and it runs the correct commands in the correct order on that graph, and it stores the targets in a hidden cache.

---

## Compare models.


```r
readd(metrics) # See also loadd()
```

&lt;img src="index_files/figure-html/c7-1.png" width="80%" height="80%" style="display: block; margin: auto;" /&gt;

???

After that, you can retrieve any target at any time with convenient interface functions `loadd()` and `readd()`.

---

## Add a new model.


```r
plan &lt;- drake_plan(
  rec = prepare_recipe(data),
  model = target(
    train_model(rec, act1 = act),
    format = "keras",
*   transform = map(act = c("relu", "sigmoid", "softmax"))
  ),
  conf = target(
    confusion_matrix(data, rec, model),
    transform = map(model, .id = act)
  ),
  metrics = target(
    compare_models(conf),
    transform = combine(conf)
  ),
  data = read_csv(
    file_in("data/customer_churn.csv"),
    col_types = cols()
  ) %&gt;%
    initial_split(prop = 0.3)
)
```



???

After you explore a bit, then it comes time for changes. Here, we go back and add another model and another confusion matrix to try to outperform the other models. We could have gone back and changed our commands and functions too.

---

## vis_drake_graph()

&lt;center&gt;
&lt;img align="middle" src = "./images/vis2.png" style="border: none; box-shadow: none;"&gt;
&lt;/center&gt;

???

But whatever we do, drake looks at all of that and decides which targets are up to date and which are outdated or missing. And there are several interface functions to display this information in different ways.

Here, since we added a new model, that model and everything downstream are out of date. But the green targets are still up to date because we haven't changed any **functions** or files or commands or upstream targets or random number generator seeds they depend on.

---

## Refresh the results in make.R.


```r
source("R/packages.R")
source("R/functions.R")
*source("R/plan.R") # modified

make(plan)
## target model_softmax
## target conf_softmax
## target metrics
```

???

When we run the new plan, we only build the outdated or missing targets. We run only **part** of the work, not all of it. Small changes to code or data no longer always invalidate everything. We save a ton of time this way.

In computing, we like to talk a lot about speed. How do we make our code run faster? How do we make it more efficient? But the fastest code is the code we don't run at all.

---

## Compare models.


```r
readd(metrics)
```

&lt;img src="index_files/figure-html/c7updated-1.png" width="80%" height="80%" style="display: block; margin: auto;" /&gt;

???

And with minimal effort, our metrics are now up to date. The plot has all three models now.

---

## Evidence of reproducibility


```r
source("R/packages.R")
source("R/functions.R")
source("R/plan.R")

make(plan)
## All targets are already up to date.
```

- See also `outdated()`.

???

If we run the `make()` function again without having changed anything, `drake` tells us that everything is up to date and does nothing else.

This is *tangible evidence* that your output matches the code and data it came from. It is *tangible evidence* of reproducibility because it increases our ability to trust our results. Reproducibility can mean a lot of different things, but as Gabe Becker likes to say, it all comes down to trust.

---

## Efficient data formats

- Increased speed and reduced memory consumption.


```r
library(drake)
n &lt;- 1e8 # Each target is 1.6 GB in memory.
plan &lt;- drake_plan(
  data_fst = target(
    data.frame(x = runif(n), y = runif(n)),
*   format = "fst"
  ),
  data_old = data.frame(x = runif(n), y = runif(n))
)
make(plan)
#&gt; target data_fst
#&gt; target data_old
build_times(type = "build")
#&gt; # A tibble: 2 x 4
#&gt;   target   elapsed              user                 system    
#&gt;   &lt;chr&gt;    &lt;Duration&gt;           &lt;Duration&gt;           &lt;Duration&gt;
*#&gt; 1 data_fst 13.93s               37.562s              7.954s
*#&gt; 2 data_old 184s (~3.07 minutes) 177s (~2.95 minutes) 4.157s
```

???

Now to shift gears and touch on some new features. You may have heard most of this from me before, so I'll try to share something newer.

drake now has special formats for more efficient data processing: one for data frames, one for data tables, one for Keras models, and an alternative RDS format that uses less memory than drake's default caching. And I'm looking to add more, so please let me know if you have ideas.

---

## History and provenance


```r
drake_history()
## # A tibble: 10 x 10
##    target  current built  exists hash  command     seed runtime  prop act1 
##    &lt;chr&gt;   &lt;lgl&gt;   &lt;chr&gt;  &lt;lgl&gt;  &lt;chr&gt; &lt;chr&gt;      &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;
##  1 conf_r… TRUE    2019-… TRUE   a945… confusio… 4.05e8  0.253   NA   &lt;NA&gt; 
##  2 conf_s… TRUE    2019-… TRUE   a18b… confusio… 1.93e9  0.206   NA   &lt;NA&gt; 
##  3 conf_s… TRUE    2019-… TRUE   5f47… confusio… 1.80e9  0.238   NA   &lt;NA&gt; 
##  4 data    TRUE    2019-… TRUE   ca84… "read_cs… 1.29e9  0.036    0.3 &lt;NA&gt; 
##  5 metrics FALSE   2019-… TRUE   f183… compare_… 1.21e9  0.019   NA   &lt;NA&gt; 
##  6 metrics TRUE    2019-… TRUE   86cb… compare_… 1.21e9  0.0190  NA   &lt;NA&gt; 
##  7 model_… TRUE    2019-… TRUE   9f3b… "train_m… 1.47e9  6.62    NA   relu 
##  8 model_… TRUE    2019-… TRUE   ca44… "train_m… 1.26e9  4.62    NA   sigm…
##  9 model_… TRUE    2019-… TRUE   6896… "train_m… 8.05e8  4.30    NA   soft…
## 10 rec     TRUE    2019-… TRUE   7605… prepare_… 6.29e8  0.120   NA   &lt;NA&gt;
```

???

`drake` also tracks history and provenance. Up to date or not, you can see what you built, when you built it, how much time it took, and the top-level function arguments you used in your commands. Best of all, you can see multiple versions of multiple targets across the whole timeline of your project.

---

## Reproducible data recovery


```r
clean() # Oops!

start &lt;- proc.time()
make(plan, recover = TRUE)
## recover data
## recover rec
## recover model_relu
## recover model_sigmoid
## recover model_softmax
## recover conf_relu
## recover conf_sigmoid
## recover conf_softmax
## recover metrics

proc.time() - start
##    user  system elapsed 
##   0.081   0.000   0.081
```

- Details + how to rename a target: &lt;https://ropenscilabs.github.io/drake-manual/walkthrough.html#reproducible-data-recovery-and-renaming&gt;

???

And depending on the conditions under which those old targets were built, you can recover them instead of building them all over again. Sometimes, the command, the upstream targets, the functions, and the random number generator seed are the same as for some run from the past. And in those cases, you can save extra time with data recovery.

Unfortunately, this is not the default because it assumes you are always using the same packages and computing environment. `drake` doesn't try to get involved with those things. To lock down your package dependencies and all that, `renv` and Docker are great tools. Not only are they fully compatible with `drake`, they compensate for some of its biggest relative weaknesses.

---

## Dependency-aware high-performance computing

- Just a little configuration...


```r
# template file with configuration
drake_hpc_template_file("slurm_clustermq.tmpl")

# Use SLURM resource manager with the template.
options(
  clustermq.scheduler = "slurm",
  clustermq.template = "slurm_clustermq.tmpl"
)

# make() is the basically the same.
make(plan, jobs = 2, parallelism = "clustermq")
```

???

And here's another feature that's been around for a long time: parallel computing. drake automatically decides which targets can run at the same time and which need to wait for dependencies. How it works is you declare the number of workers, the parallel backend, and resource requirements if you're on a cluster...

---

## Dependency-aware high-performance computing

&lt;iframe width="800" height="450" src="https://www.powtoon.com/embed/bUfSIaXjrw5/" frameborder="0"&gt;&lt;/iframe&gt;

???

And drake takes care of the rest. It launches a bunch of workers and sends those workers to the targets as soon as they're ready to go. It automatically accounts for the fact that some targets depend on others. 

---

## Resources

- Get [`drake`](https://github.com/ropensci/drake):


```r
install.packages("drake")
```

- Example code from these slides:


```r
drake::drake_example("customer-churn")
```

- Workshop materials:


```r
remotes::install_github("wlandau/learndrake")
```

???

Here's how to learn more about drake. Just as drake itself exists and operates entirely within R, so do many of its resources.

---

## Links

- Development repository: &lt;https://github.com/ropensci/drake&gt;
- Full user manual &lt;https://ropenscilabs.github.io/drake-manual&gt;
- Reference website: &lt;https://docs.ropensci.org/drake&gt;
- Hands-on workshop: &lt;https://github.com/wlandau/learndrake&gt;
- Code examples: &lt;https://github.com/wlandau/drake-examples&gt;
- Discuss at rOpenSci.org: &lt;https://discuss.ropensci.org&gt;

## rOpenSci use cases

- Use [`drake`](https://github.com/ropensci/drake)? Share your use case at &lt;https://ropensci.org/usecases&gt;.

&lt;center&gt;
&lt;img src = "./images/ropensci.png" style="border: none; box-shadow: none; height: 150px"&gt;
&lt;/center&gt;

???

For the online stuff, I highly recommend the "learndrake" workshop. Just go to the GitHub page and click the blue "launch binder" badge in the README, and suddenly you're in RStudio Server with everything ready to go. Karthik's `holepunch` package made setting that up really smooth.

And if you use `drake` for a real project, please consider writing about it as an rOpenSci use case. It enhances these resources, it demonstrates the value of rOpenSci packages, and it grows the community.

---

## Thanks

&lt;br&gt;
&lt;br&gt;
&lt;table style = "border: none"&gt;
&lt;tr&gt;
&lt;td style = "padding-right: 125px"&gt;
&lt;ul style&gt;
&lt;img src = "./images/edgar.jpg" style="border: none; box-shadow: none; height: 150px"&gt;
&lt;li&gt;&lt;a href = "https://github.com/edgararuiz"&gt;Edgar Ruiz&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href = "https://github.com/sol-eng/tensorflow-w-r/blob/master/workflow/tensorflow-drake.Rmd"&gt;example code&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;ul&gt;
&lt;img src = "./images/matt.jpg" style="border: none; box-shadow: none; height: 150px"&gt;
&lt;li&gt;&lt;a href = "https://github.com/mdancho84"&gt;Matt Dancho&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href = "https://blogs.rstudio.com/tensorflow/posts/2018-01-11-keras-customer-churn/"&gt;blog post&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

???

Today, the deep learning example came from Matt Dancho and Edgar Ruiz.

---

## Thanks

&lt;table style = "border: none"&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;br&gt;
&lt;ul&gt;
&lt;img src = "./images/ropensci.png" style="border: none; box-shadow: none; height: 150px"&gt;
&lt;li&gt;&lt;a href = "https://github.com/maelle"&gt;Maëlle Salmon&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href = "https://github.com/benmarwick"&gt;Ben Marwick&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href = "https://github.com/jules32"&gt;Julia Lowndes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href = "https://github.com/gothub"&gt;Peter Slaughter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href = "https://github.com/jennybc"&gt;Jenny Bryan&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href = "https://github.com/richfitz"&gt;Rich FitzJohn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href = "https://github.com/stefaniebutland"&gt;Stefanie Butland&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href = "https://github.com/jarad"&gt;Jarad Niemi&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href = "https://github.com/krlmlr"&gt;Kirill Müller&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href = "https://github.com/HenrikBengtsson"&gt;Henrik Bengtsson&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href = "https://github.com/mschubert"&gt;Michael Schubert&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href = "https://github.com/kendonB"&gt;Kendon Bell&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href = "https://github.com/milesmcbain"&gt;Miles McBain&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href = "https://github.com/pat-s"&gt;Patrick Schratz&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href = "https://github.com/AlexAxthelm"&gt;Alex Axthelm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href = "https://github.com/dapperjapper"&gt;Jasper Clarkberg&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href = "https://github.com/tiernanmartin"&gt;Tiernan Martin&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href = "https://github.com/BListyg"&gt;Ben Listyg&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href = "https://github.com/tjmahr"&gt;TJ Mahr&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href = "https://github.com/bpbond"&gt;Ben Bond-Lamberty&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href = "https://github.com/tmastny"&gt;Tim Mastny&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href = "https://github.com/billdenney"&gt;Bill Denney&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href = "https://github.com/aedobbyn"&gt;Amanda Dobbyn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href = "https://github.com/dfalster"&gt;Daniel Falster&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href = "https://github.com/rkrug"&gt;Rainer Krug&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href = "https://github.com/bmchorse"&gt;Brianna McHorse&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href = "https://github.com/mrchypark"&gt;Chan-Yub Park&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

???

For the development of drake itself, I have a lot of people to thank, many more than are on this list.

rOpenSci in particular is the one of the friendliest and most welcoming communities in tech, an amazing combination of expertise and approachability. `drake` is a better package and I am a better developer because they coached me during the software review process, they got the word out, and they connected me to users with insightful ideas and developers who I look up to.

---

## A riddle!

- From a math PhD oral exam:

&gt; Give an example of a nontrivial function.

- Hint: the best answers do not even come from math or computing!

&lt;center&gt;
&lt;img src = "./images/pythagoras.png" align="middle" style="border: none; box-shadow: none; height: 375px; text-align: center;"&gt;
&lt;div style="font-size: 0.5em; text-align: center"&gt;&lt;a href="https://publicdomainvectors.org/en/free-clipart/Pythagoras-tree/58775.html"&gt;https://publicdomainvectors.org/en/free-clipart/Pythagoras-tree/58775.html&lt;/a&gt;&lt;/div&gt;
&lt;/center&gt;

???

Let's end the talk portion with a riddle that totally stumped me. For fun, try to think of an example of a highly nontrivial function. I heard this was originally part of a math PhD oral exam, but apparently the best answers are completely unrelated to math or any kind of STEM field. I do have an answer in mind, but I'm going to wait to share it.

I recommend taking something you already think is nontrivial, no matter what it is, and try turning it into a function. I'm curious what people come up with, and I think it could be a good exercise to help people use functions more often.
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightLines": true
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
