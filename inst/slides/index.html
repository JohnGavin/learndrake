<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Reproducible workflows at scale with drake</title>
    <meta charset="utf-8" />
    <meta name="author" content="Will Landau" />
    <link href="index_files/remark-css/default.css" rel="stylesheet" />
    <link href="index_files/remark-css/default-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Reproducible workflows at scale with drake
### Will Landau

---


&lt;style&gt;
.inverse {
background-color: transparent;
text-shadow: 0 0 0px transparent;
}
.title-slide {
vertical-align: bottom !important; 
text-align: center !important;
}
.title-slide h1 {
position: absolute;
top: 0;
left: 0;
right: 0;
width: 100%;
line-height: 4em;
color: #666666;
}
.title-slide h3 {
line-height: 6em;
color: #666666;
}
.title-slide {
background-color: white;
background-image: url('images/logo.png');
background-repeat: no-repeat;
background-size: 25%;
}
.remark-slide-content:after {
content: "Copyright Eli Lilly and Company";
position: absolute;
bottom: -5px;
left: 20px;
height: 40px;
width: 100%;
font-family: Helvetica, Arial, sans-serif;
font-size: 0.7em;
color: gray;
background-repeat: no-repeat;
background-size: contain;
}
&lt;/style&gt;



## Large data science workflows

- Struggles

  1. Long runtimes.
  2. Many tasks.
  3. Interconnected tasks.

- Examples
  - Deep learning.
  - Classical machine learning.
  - Bayesian data analysis via Markov chain Monte Carlo.
  - Spatial data analysis.
  - Clinical trial modeling and simulation.
  - Subgroup identification.
  - Graph-based multiple comparison procedures.
  - Genomics pipelines.
  - PK/PD modeling.

???

Let's talk about what happens in large data science projects. Projects that are ambitious and complicated, that have a lot of moving parts and long runtimes. Projects that are difficult just to wrap your head around, that are difficult just to run from start to finish, but whose results really matter and carry important consequences.

---

## Interconnected tasks
&lt;center&gt;
&lt;img src = "./images/workflow.png" align="middle" style="border: none; box-shadow: none; text-align: center;"&gt;
&lt;/center&gt;

???

The tasks of a data analysis project are interconnected.

---

## When you change something...
&lt;center&gt;
&lt;img src = "./images/change.png" align="middle" style="border: none; box-shadow: none; text-align: center;"&gt;
&lt;/center&gt;

???

When you go back and change something...

---

## ...the downstream output is **no longer valid**.

&lt;center&gt;
&lt;img src = "./images/downstream.png" align="middle" style="border: none; box-shadow: none; text-align: center;"&gt;
&lt;/center&gt;

???

...everything that depends on it is no longer valid. And you need to pick up the pieces.

---

## Do you rerun **everything** from scratch?

- Not if you deal with long runtimes!

&lt;center&gt;
&lt;img src = "./images/sisyphus.svg" align="middle" style="border: none; box-shadow: none; height: 375px; text-align: center;"&gt;
&lt;div style="font-size: 0.5em; text-align: center"&gt;&lt;a href="https://openclipart.org/detail/275842/sisyphus-overcoming-silhouette"&gt;https://openclipart.org/detail/275842/sisyphus-overcoming-silhouette&lt;/a&gt;&lt;/div&gt;
&lt;/center&gt;

???

For large projects, this is where the programming and workflow management techniques we are used to start to break down. If it takes several hours just to fit a single model, you're not going to want to rerun your whole analysis every time you make a change. It's just too much frustration.

But I used to do this all the time. In my dissertation work, every model took about 4 hours, and I needed results from about 20 or 30 of them. I spent the last 6 months of grad school mostly restarting code and waiting it to finish. My results were out of sync with the code, and it was hard to write my thesis that way.

---

## Do you pick and choose what to update?

- Messy.
- Prone to human error.
- Not reproducible.

&lt;center&gt;
&lt;img src = "./images/mess.svg" align="middle" style="border: none; box-shadow: none; height: 400px; text-align: center;"&gt;
&lt;div style="font-size: 0.5em; text-align: center;"&gt;&lt;a href="https://openclipart.org/detail/216179/messy-desk"&gt;https://openclipart.org/detail/216179/messy-desk&lt;/a&gt;&lt;/div&gt;
&lt;/center&gt;

???

And it's perilous to try to stop this cycle on your own without any help. If I'm the one deciding which code to skip and which code to rerun, it's a mess. It's too easy to make big mistakes, and the results are not reproducible. I would not trust a human to do this.

---

## Solution: pipeline tools

&lt;center&gt;
&lt;img src = "./images/infographic.svg" align="middle" style="border: none; box-shadow: none; text-align: center;"&gt;
&lt;/center&gt;

- Tons exist already: [github.com/pditommaso/awesome-pipeline](https://github.com/pditommaso/awesome-pipeline).
- Most are language-agnostic or designed for Python or the shell.

???

Fortunately, we can *automate* the process of bringing results up to date as fast as possible. The standard solution is a pipeline tool. You may have heard of examples already. There's

- Make
- Snakemake
- Airflow
- Nextflow
- Luigi
- Dask

But they're difficult to use with R. Most of them try to be language-agnostic for the sake of versatility, but this means they add extra friction when you try to work with any one language in particular. My experience with Make specifically is that it obstructs my relationship with R. It pulls me out of the language and into sub-optimal programming practices.

---

## What distinguishes `drake`?

&lt;center&gt;
&lt;img src = "./images/R.png" align="middle" style="border: none; box-shadow: none; text-align: center; height: 200px"&gt;
&lt;/center&gt;

- Aggressively designed for R.
1. Think **functions**, not script files.
2. Think **variables**, not output files.
3. Think **data frames**, not `Makefile`s.
- [`drake`](https://github.com/ropensci/drake) borrows (1) and (2) from the [`remake`](https://github.com/richfitz/remake) package by [Rich FitzJohn](https://github.com/richfitz).
- [`remake`](https://github.com/richfitz/remake) is no longer under development.
- [`drake`](https://github.com/ropensci/drake) tries to extend [`remake`](https://github.com/richfitz/remake)'s ideas further and handle larger projects.

???


drake, on the other hand, is designed for R at its very core. It works entirely *within* the language, and it nudges you to write good code you can trust.

1. Instead of imperative script files, you write **functions**.

2. Instead of saving output files manually and dealing with all the headache of trying to organize them yourself, you return **objects** from those functions, and drake saves them for you.

3. And instead of Makefiles in Make or YAML files in remake, you have an R-focused interface to define the skippable steps of your workflow.

By the way, drake was not the first pipeline tool to respect the way R works. Before drake, there was a package called `remake` by Rich FitzJohn. remake was playing nicely with objects and functions long before I created drake.

But remake is no longer maintained. And because it required you to work with YAML configuration files and supply your actual R scripts directly, it kind of had one foot in and one foot out anyway. drake tries to finish what remake started and scale up the intensity of work you can do with it.

---

## Example: a deep learning workflow

- Goal: predict customers who cancel their subscriptions with a telecom company.
- Data: [IBM Watson Telco Customer Churn dataset](https://www.ibm.com/communities/analytics/watson-analytics-blog/predictive-insights-in-the-telco-customer-churn-data-set/).
- Workflow principles generalize to other industries.

&lt;img src = "./images/combine.png" style="border: none; box-shadow: none; height: 200px"&gt;

&lt;div style="font-size: 0.5em;"&gt;&lt;a href="https://openclipart.org/detail/90739/newplus"&gt;https://openclipart.org/detail/90739/newplus&lt;/a&gt;, &lt;a href="https://github.com/rstudio/keras"&gt;https://github.com/rstudio/keras&lt;/a&gt;&lt;/div&gt;

???

To dive into drake, we're going to use a machine learning example. We've got a deep neural net, and we're going to use it to predict "customer churn", which is another way of saying attrition, or dropout.

---

background-image: ./images/not.png

## &lt;img src="./images/no.png" width="40" height="40"&gt; Let's move beyond numbered scripts.


```r
run_everything.R
R/
├── 01-data.R
├── 02-munge.R
├── 03-model.R
├── 04-results.R
└── 05-plot.R
data/
└── customer_churn.csv
output/
├── model_relu.h5
├── model_sigmoid.h5
├── confusion_matrix.rds
└── metrics_plot.png
```

???

Projects like this tend to get too big for the classic "numbered script" paradigm you may have seen. It falls apart pretty easily.

---

## &lt;img src="./images/no.png" width="40" height="40"&gt; Why not numbered scripts?

- The planning and the execution happen at the same time.
- Too cumbersome, ad hoc, and tangled for ambitious projects.


```r
# 02-munge.R
*library(recipes) # Package dependencies scattered across scripts.

*rec &lt;- data %&gt;% # Single-use code, difficult to test.
  training() %&gt;%
  recipe(Churn ~ .) %&gt;%
  step_rm(customerID) %&gt;%
  step_naomit(all_outcomes(), all_predictors()) %&gt;%
  step_discretize(tenure, options = list(cuts = 6)) %&gt;%
  step_log(TotalCharges) %&gt;%
  step_mutate(Churn = ifelse(Churn == "Yes", 1, 0)) %&gt;%
  step_dummy(all_nominal(), -all_outcomes()) %&gt;%
  step_center(all_predictors(), -all_outcomes()) %&gt;%
  step_scale(all_predictors(), -all_outcomes()) %&gt;%
  prep()

*saveRDS(rec, "recipe.rds") # Final output scattered across code.
```

???

And the reason is that it tries to do too many things at once. The planning and the execution happen at the same time, and the actual content of the analysis is muddled with the bookkeeping.

In these scripts, you've got package dependencies and file-saving steps scattered everywhere. And for this preprocessing recipe, it looks like one thing, but it's really two different things. You're defining a complicated procedure, and you're running it **as** you're defining it. These things should really be separate. Otherwise, it's hard to take the code out of context and test it.

But more importantly, we don't have shorthand names for the long complicated steps we're doing. We don't have the structure to organize the ideas, so it's hard to take a step back and think about what we're doing.

In the R community, we like to criticize Microsoft Excel. We give it a hard time for putting the formulas and the spreadsheets in the same file, and we claim it's not reproducible. And we have a point. But we are guilty of something very similar! We have a habit of knitting and weaving and tangling too many things together that don't belong together!

---

## &lt;img src="./images/yes.png" width="60" height="40"&gt; Instead, embrace **functions**!

- A function is a reusable command that accepts one or more inputs and returns a single output.


```r
my_function &lt;- function(argument1, argument2) {
  argument1 + argument2
}

my_function(1, 2)
## [1] 3

my_function(3, 4)
## [1] 7
```

???

We can do better. In computing, the easiest, most ubiquitous, most standard way to solve this problem is to write functions.

A function is just a reusable command that accepts one or more inputs and returns a single output. You define it once, you give it a name, and you call it whenever you need it on whatever data that fits.

---

## Why use functions?

1. Clarity: break down complicated ideas into manageable pieces.
2. Use R as intended.

    &gt;    - Everything that exists is an object.
    &gt;    - Everything that happens is a function call.
    &gt;
    &gt; John Chambers

3. Reuse: define once, run wherever.

???

But functions do so much more than that. They're not just for code you want to repeat and reuse. They're for code you want to **understand**.

Functions break down complicated ideas into manageable pieces. Instead of a huge mess, you have components that are smaller, cleaner, less intimidating, and that mean something clear and useful.

Functions are like paragraphs. It's the same mental experience inside your head. You wouldn't write an endless wall of text as a paper or an email, right? It's the same thing here, and R Markdown chunks don't have the structure to get us there. With functions, you're crystallizing a stream of consciousness into clear, discrete, explainable, composable, chainable points you want to make.

Look, nothing is going to change the fact that writing code is difficult. It will always be difficult. For everyone. Not even the Tidyverse or R Markdown can replace the fact that every single one of us has to concentrate and get our thoughts in order. Feeling the strain is normal. It does **not** make you an imposter, it just makes you self-aware. And the more we lean into that, the better off we are in the long run. 

Things start to make more sense once you do this. For example, you'll notice that you're being true to the process of data analysis itself. Functions are **fundamentally designed** to describe change. And in our workflows, we change raw data into clean data, clean data into model output, and so on. Those steps are begging to be functions.

You're also being true to the R language. You're using it how it's **designed** to be used, and I think that John Chambers quote says it best.

---

## Functions in a workflow


```r
make.R
R/
├── packages.R
*├── functions.R
└── plan.R
data/
└── customer_churn.csv
.drake/ # drake's cache
└──     # Output automatically appears here.
```

???

So let's use functions in the deep learning example. Most the real estate in our scripts is functions.

---

## Functions in a workflow


```r
*# packages.R: all package dependencies
library(recipes)
# other packages...
```


```r
*# functions.R: pure reusable code
prepare_recipe &lt;- function(data) {
  data %&gt;%
    training() %&gt;%
    recipe(Churn ~ .) %&gt;%
    step_rm(customerID) %&gt;%
    step_naomit(all_outcomes(), all_predictors()) %&gt;%
    step_discretize(tenure, options = list(cuts = 6)) %&gt;%
    step_log(TotalCharges) %&gt;%
    step_mutate(Churn = ifelse(Churn == "Yes", 1, 0)) %&gt;%
    step_dummy(all_nominal(), -all_outcomes()) %&gt;%
    step_center(all_predictors(), -all_outcomes()) %&gt;%
    step_scale(all_predictors(), -all_outcomes()) %&gt;%
    prep()
}
# other functions...
```

???

Our preprocessing task is a single, clear, distinct idea with clearly-defined inputs and a single return value. So we put it in a function.

---

## Functions in a workflow


```r
# later in functions.R...

run_everything &lt;- function() {
  data &lt;- read_csv(file_in("data/customer_churn.csv"), col_types = cols()) %&gt;%
    initial_split(prop = 0.3)
  saveRDS(data, "output/data.rds")

* rec &lt;- prepare_recipe(data) # Call your other functions.
  saveRDS(rec, "output/rec.rds")

  model_relu &lt;- train_model(rec, act1 = "relu")
  save_model_hdf5(model_relu, "output/model_relu.h5")
  # more models...

  conf_sigmoid &lt;- confusion_matrix(data, rec, model_sigmoid)
  saveRDS(conf_sigmoid, "output/conf_sigmoid.rds")
  # more confusion matrices...

  metrics &lt;- compare_models(conf_relu, conf_sigmoid)
  saveRDS(metrics, "output/metrics.rds")
}
```

???

And we can take the functions we define and call them from other functions, including a function that runs everything at the top level and controls the flow of the analysis.

---

## Conduct your analysis with your **functions**.


```r
# run_everything.R
source("R/packages.R")
source("R/functions.R")
run_everything()
```

???

Our top-level **script** pretty much just calls our flagship function. And that's a huge improvement.

---

## But we can still do better...

- Avoid rerunning all the computation every time.
- Avoid micromanaging output files.

&lt;center&gt;
&lt;img src = "./images/genie.png" align="middle" style="border: none; box-shadow: none; height: 375px; text-align: center;"&gt;
&lt;div style="font-size: 0.5em; text-align: center"&gt;&lt;a href="https://publicdomainvectors.org/en/free-clipart/Golden-magic-lamp/61683.html"&gt;https://publicdomainvectors.org/en/free-clipart/Golden-magic-lamp/61683.html&lt;/a&gt;&lt;/div&gt;
&lt;/center&gt;

???

But we can still do better. We can avoid rerunning all the computation every time. We can avoid micromanaging output files. And that's where drake helps us.

---

## Enter drake! Define a **plan**.


```r
plan &lt;- drake_plan(
* rec = prepare_recipe(data), # Use your functions.
  model = target(
    train_model(rec, act1 = act),
    format = "keras",
    transform = map(act = c("relu", "sigmoid"))
  ),
  conf = target(
    confusion_matrix(data, rec, model),
    transform = map(model, .id = act)
  ),
  metrics = target(
    compare_models(conf),
    transform = combine(conf)
  ),
* data = read_csv(                      # flexible target order,
*   file_in("data/customer_churn.csv"), # flexible commands
    col_types = cols()
  ) %&gt;%
    initial_split(prop = 0.3)
)
```

???

Instead of a `run_everything()` function, we create a drake plan. The plan defines a bunch of skippable steps. Each step has a command, and each command returns a value called a **target**. We give these targets names on the left, and we list them in any order we want. So we can even define the data at the bottom here.

---

## The plan is a data frame of skippable tasks.


```r
plan
## # A tibble: 7 x 3
##   target      command                                                format
##   &lt;chr&gt;       &lt;expr&gt;                                                 &lt;chr&gt; 
## 1 rec         prepare_recipe(data)                                 … &lt;NA&gt;  
## 2 model_relu  train_model(rec, act1 = "relu")                      … keras 
## 3 model_sigm… train_model(rec, act1 = "sigmoid")                   … keras 
## 4 conf_relu   confusion_matrix(data, rec, model_relu)              … &lt;NA&gt;  
## 5 conf_sigmo… confusion_matrix(data, rec, model_sigmoid)           … &lt;NA&gt;  
## 6 metrics     compare_models(conf_relu, conf_sigmoid)              … &lt;NA&gt;  
## 7 data        read_csv(file_in("data/customer_churn.csv"), col_type… &lt;NA&gt;
```

???

This plan is a tidy data frame of commands and targets. This is what you get instead of a Makefile. It exists entirely within R, and it uses a data structure we all like.

---

## The workflow

&lt;br&gt;
&lt;center&gt;
&lt;img align="middle" src = "./images/vis1.png" style="border: none; box-shadow: none;"&gt;
&lt;/center&gt;

???

Now `drake` takes this, and it analyzes your plan, and it analyzes your functions, without running any of your code just yet. It detects the symbols that you mention and any optional input files you choose to declare, and it decides which steps go first and which steps go last. Because it analyzes your code, drake can figure this out regardless of the order you wrote the targets in the plan. For example, the "rec" target comes after the data because the command for "rec" mentions the symbol "data".

---

## Run the project in make.R.


```r
# like run_everything.R...
source("R/packages.R")
source("R/functions.R")
source("R/plan.R")

make(plan)
## target data
## target rec
## target model_relu
## target model_sigmoid
## target conf_relu
## target conf_sigmoid
## target metrics
```

???

With all that in place, we can actually run the workflow now. We source the scripts to load our packages, functions, and plan, and we call the `make()` function to actually execute the tasks. The `make()` function runs the correct commands in the correct order using that graph, and it stores the targets in a hidden cache.

---

## Compare models.


```r
readd(metrics) # See also loadd()
```

&lt;img src="index_files/figure-html/c7-1.png" width="80%" height="80%" style="display: block; margin: auto;" /&gt;

???

After that, you can retrieve any target at any time with convenient interface functions `loadd()` and `readd()`.

---

## Add a new model.


```r
plan &lt;- drake_plan(
  rec = prepare_recipe(data),
  model = target(
    train_model(rec, act1 = act),
    format = "keras",
*   transform = map(act = c("relu", "sigmoid", "softmax"))
  ),
  conf = target(
    confusion_matrix(data, rec, model),
    transform = map(model, .id = act)
  ),
  metrics = target(
    compare_models(conf),
    transform = combine(conf)
  ),
  data = read_csv(
    file_in("data/customer_churn.csv"),
    col_types = cols()
  ) %&gt;%
    initial_split(prop = 0.3)
)
```



???

drake is designed for changes. Here, we go back and add another model and another confusion matrix to try to outperform the others. We could have gone back and changed our commands and functions too.

---

## vis_drake_graph()

&lt;center&gt;
&lt;img align="middle" src = "./images/vis2.png" style="border: none; box-shadow: none;"&gt;
&lt;/center&gt;

???

But whatever we do, drake looks at all of that and decides which targets are up to date and which are outdated or missing. And there are several interface functions to display this information in different ways.

Here, since we added a new model, that model and everything downstream are out of date. But the green targets are still up to date because we haven't changed any **functions** or files commands or upstream targets or random number generator seeds they depend on.

---

## Refresh the results in make.R.


```r
source("R/packages.R")
source("R/functions.R")
*source("R/plan.R") # modified

make(plan)
## target model_softmax
## target conf_softmax
## target metrics
```

???

When we run the new plan, we only build the outdated or missing targets. We run only **part** of the work, not all of it. Small changes to code or data no longer always invalidate everything. We save a ton of time this way.

In computing, we like to talk a lot about speed. How do we make our code run faster? How do we make it more efficient? But the fastest code is the code we don't run at all.

---

## Compare models.


```r
readd(metrics)
```

&lt;img src="index_files/figure-html/c7updated-1.png" width="80%" height="80%" style="display: block; margin: auto;" /&gt;

???

And with minimal effort, our metrics are now up to date. The plot has all three models now.

---

## Evidence of reproducibility


```r
source("R/packages.R")
source("R/functions.R")
source("R/plan.R")

make(plan)
## All targets are already up to date.
```

- See also `outdated()`.

???

If we run the `make()` function again without having changed anything, `drake` tells us that everything is up to date and does nothing else.

This is *tangible evidence* that your output matches the code and data it came from. It is *tangible evidence* of reproducibility.

Reproducibility can mean a lot of different things.

- Replicability at the lab bench,
- The amount of data you share,
- How well you explain your methods,
- How often your method has been replicated,
- How credentialed the authors are.

But as Gabe Becker likes to say, it all comes down to trust. Reproducibility is about trust. And drake can be part of the validation that goes into all that.

---

## Efficient data formats

- Increased speed and reduced memory consumption.


```r
library(drake)
n &lt;- 1e8 # Each target is 1.6 GB in memory.
plan &lt;- drake_plan(
  data_fst = target(
    data.frame(x = runif(n), y = runif(n)),
*   format = "fst"
  ),
  data_old = data.frame(x = runif(n), y = runif(n))
)
make(plan)
#&gt; target data_fst
#&gt; target data_old
build_times(type = "build")
#&gt; # A tibble: 2 x 4
#&gt;   target   elapsed              user                 system    
#&gt;   &lt;chr&gt;    &lt;Duration&gt;           &lt;Duration&gt;           &lt;Duration&gt;
*#&gt; 1 data_fst 13.93s               37.562s              7.954s
*#&gt; 2 data_old 184s (~3.07 minutes) 177s (~2.95 minutes) 4.157s
```

???

Now for some new features.

drake now has special formats for more efficient data processing: one for data frames, one for data tables, and one for Keras models. And I'm looking to add more.

---

## History and provenance


```r
drake_history()
## # A tibble: 10 x 10
##    target  current built  exists hash  command     seed runtime  prop act1 
##    &lt;chr&gt;   &lt;lgl&gt;   &lt;chr&gt;  &lt;lgl&gt;  &lt;chr&gt; &lt;chr&gt;      &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;
##  1 conf_r… TRUE    2019-… TRUE   4720… confusio… 4.05e8  0.232   NA   &lt;NA&gt; 
##  2 conf_s… TRUE    2019-… TRUE   9137… confusio… 1.93e9  0.187   NA   &lt;NA&gt; 
##  3 conf_s… TRUE    2019-… TRUE   00c3… confusio… 1.80e9  0.260   NA   &lt;NA&gt; 
##  4 data    TRUE    2019-… TRUE   ca84… "read_cs… 1.29e9  0.036    0.3 &lt;NA&gt; 
##  5 metrics FALSE   2019-… TRUE   f043… compare_… 1.21e9  0.0200  NA   &lt;NA&gt; 
##  6 metrics TRUE    2019-… TRUE   d18f… compare_… 1.21e9  0.0310  NA   &lt;NA&gt; 
##  7 model_… TRUE    2019-… TRUE   581e… "train_m… 1.47e9  6.37    NA   relu 
##  8 model_… TRUE    2019-… TRUE   7b46… "train_m… 1.26e9  4.10    NA   sigm…
##  9 model_… TRUE    2019-… TRUE   945a… "train_m… 8.05e8  4.47    NA   soft…
## 10 rec     TRUE    2019-… TRUE   7605… prepare_… 6.29e8  0.127   NA   &lt;NA&gt;
```

???

`drake` also tracks history and provenance. You can see useful information about all the versions of old targets from the past.

---

## Reproducible data recovery


```r
clean() # Oops!

start &lt;- proc.time()
make(plan, recover = TRUE)
## recover data
## recover rec
## recover model_relu
## recover model_sigmoid
## recover model_softmax
## recover conf_relu
## recover conf_sigmoid
## recover conf_softmax
## recover metrics

proc.time() - start
##    user  system elapsed 
##   0.083   0.000   0.082
```

- Details + how to rename a target: &lt;https://ropenscilabs.github.io/drake-manual/walkthrough.html#reproducible-data-recovery-and-renaming&gt;

???

And depending on the conditions under which those old targets were built, you can recover them instead of building them. Another time-saver.

Unfortunately, this is not the default because it assumes you are always using the same packages and computing environment. To lock that part down, `renv` and Docker are great tools, and they're compatible with `drake`.

---

## Dependency-aware high-performance computing

- Just a little configuration...


```r
# template file with configuration
drake_hpc_template_file("slurm_clustermq.tmpl")

# Use SLURM resource manager with the template.
options(
  clustermq.scheduler = "slurm",
  clustermq.template = "slurm_clustermq.tmpl"
)

# make() is the basically the same.
make(plan, jobs = 2, parallelism = "clustermq")
```

???

And here's another feature that's been around for a long time: parallel computing. drake automatically decides which targets can run at the same time and which need to wait for dependencies. How it works is you declare the number of workers, the parallel backend, and resource requirements if you're on a cluster...

---

## Dependency-aware high-performance computing

&lt;iframe width="800" height="450" src="https://www.powtoon.com/embed/bUfSIaXjrw5/" frameborder="0"&gt;&lt;/iframe&gt;

???

And drake takes care of the rest. It launches a bunch of workers and sends those workers to the targets as soon as they're ready to go.

---

## Resources

- Get [`drake`](https://github.com/ropensci/drake):


```r
install.packages("drake")
```

- Example code from these slides:


```r
drake::drake_example("customer-churn")
```

- Workshop materials:


```r
remotes::install_github("wlandau/learndrake")
```

???

Here's how to learn more about drake. Just as drake itself exists and operates entirely within R, so do many of its resources.

---

## Links

- Development repository: &lt;https://github.com/ropensci/drake&gt;
- Full user manual &lt;https://ropenscilabs.github.io/drake-manual&gt;
- Reference website: &lt;https://docs.ropensci.org/drake&gt;
- Hands-on workshop: &lt;https://github.com/wlandau/learndrake&gt;
- Code examples: &lt;https://github.com/wlandau/drake-examples&gt;
- Discuss at rOpenSci.org: &lt;https://discuss.ropensci.org&gt;

## rOpenSci use cases

- Use [`drake`](https://github.com/ropensci/drake)? Share your use case at &lt;https://ropensci.org/usecases&gt;.

&lt;center&gt;
&lt;img src = "./images/ropensci.png" style="border: none; box-shadow: none; height: 150px"&gt;
&lt;/center&gt;

???

For the online stuff, I highly recommend the "learndrake" workshop. Just go to the GitHub page and click the "launch binder" badge, and suddenly you're in RStudio Server with everything ready to go.

If you use `drake` for a real project, please consider writing about it as an rOpenSci use case. It helps demonstrate the value of rOpenSci packages and grow the community.

---

## Thanks

&lt;br&gt;
&lt;br&gt;
&lt;table style = "border: none"&gt;
&lt;tr&gt;
&lt;td style = "padding-right: 125px"&gt;
&lt;ul style&gt;
&lt;img src = "./images/edgar.jpg" style="border: none; box-shadow: none; height: 150px"&gt;
&lt;li&gt;&lt;a href = "https://github.com/edgararuiz"&gt;Edgar Ruiz&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href = "https://github.com/sol-eng/tensorflow-w-r/blob/master/workflow/tensorflow-drake.Rmd"&gt;example code&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;ul&gt;
&lt;img src = "./images/matt.jpg" style="border: none; box-shadow: none; height: 150px"&gt;
&lt;li&gt;&lt;a href = "https://github.com/mdancho84"&gt;Matt Dancho&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href = "https://blogs.rstudio.com/tensorflow/posts/2018-01-11-keras-customer-churn/"&gt;blog post&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

???

Today, I borrowed the deep learning example from Matt Dancho and Edgar Ruiz.

---

## Thanks

&lt;table style = "border: none"&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;br&gt;
&lt;ul&gt;
&lt;img src = "./images/ropensci.png" style="border: none; box-shadow: none; height: 150px"&gt;
&lt;li&gt;&lt;a href = "https://github.com/maelle"&gt;Maëlle Salmon&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href = "https://github.com/benmarwick"&gt;Ben Marwick&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href = "https://github.com/jules32"&gt;Julia Lowndes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href = "https://github.com/gothub"&gt;Peter Slaughter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href = "https://github.com/jennybc"&gt;Jenny Bryan&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href = "https://github.com/richfitz"&gt;Rich FitzJohn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href = "https://github.com/stefaniebutland"&gt;Stefanie Butland&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href = "https://github.com/jarad"&gt;Jarad Niemi&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href = "https://github.com/krlmlr"&gt;Kirill Müller&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href = "https://github.com/HenrikBengtsson"&gt;Henrik Bengtsson&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href = "https://github.com/mschubert"&gt;Michael Schubert&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href = "https://github.com/kendonB"&gt;Kendon Bell&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href = "https://github.com/milesmcbain"&gt;Miles McBain&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href = "https://github.com/pat-s"&gt;Patrick Schratz&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href = "https://github.com/AlexAxthelm"&gt;Alex Axthelm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href = "https://github.com/dapperjapper"&gt;Jasper Clarkberg&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href = "https://github.com/tiernanmartin"&gt;Tiernan Martin&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href = "https://github.com/BListyg"&gt;Ben Listyg&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href = "https://github.com/tjmahr"&gt;TJ Mahr&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href = "https://github.com/bpbond"&gt;Ben Bond-Lamberty&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href = "https://github.com/tmastny"&gt;Tim Mastny&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href = "https://github.com/billdenney"&gt;Bill Denney&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href = "https://github.com/aedobbyn"&gt;Amanda Dobbyn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href = "https://github.com/dfalster"&gt;Daniel Falster&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href = "https://github.com/rkrug"&gt;Rainer Krug&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href = "https://github.com/bmchorse"&gt;Brianna McHorse&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href = "https://github.com/mrchypark"&gt;Chan-Yub Park&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

???

For the development of drake itself, I have a lot of people to thank, many more than are on this list.

rOpenSci in particular is the one of the friendliest and most welcoming communities in tech, an amazing combination of expertise and approachability. `drake` is a better package and I am a better developer because they coached me and got the word out, connected me with users with insightful ideas and developers who I look up to.

---

## A riddle!

- From a math PhD oral exam:

&gt; Give an example of a nontrivial function.

- Hint: the best answers do not even come from math or computing!

&lt;center&gt;
&lt;img src = "./images/pythagoras.png" align="middle" style="border: none; box-shadow: none; height: 375px; text-align: center;"&gt;
&lt;div style="font-size: 0.5em; text-align: center"&gt;&lt;a href="https://publicdomainvectors.org/en/free-clipart/Pythagoras-tree/58775.html"&gt;https://publicdomainvectors.org/en/free-clipart/Pythagoras-tree/58775.html&lt;/a&gt;&lt;/div&gt;
&lt;/center&gt;

???

Let's end the talk portion with a riddle. For fun, try to think of an example of a nontrivial function.

This was a question from a math PhD oral exam, but you don't have to know any fancy math to think of something cool. In fact, everything you can write with a formula or code isn't enough to solve the riddle. You have to really believe that **everything** that happens is a function call, and you have to make a function out something you might not otherwise think is possible.
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightLines": true
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
