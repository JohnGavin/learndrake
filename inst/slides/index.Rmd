---
title: "Reproducible workflows at scale with drake"
author: Will Landau
output: 
  xaringan::moon_reader:
    nature:
      highlightLines: true
---

<style>
.inverse {
background-color: transparent;
text-shadow: 0 0 0px transparent;
}
.title-slide {
vertical-align: bottom !important; 
text-align: center !important;
}
.title-slide h1 {
position: absolute;
top: 0;
left: 0;
right: 0;
width: 100%;
line-height: 4em;
color: #666666;
}
.title-slide h3 {
line-height: 6em;
color: #666666;
}
.title-slide {
background-color: white;
background-image: url('images/logo.png');
background-repeat: no-repeat;
background-size: 25%;
}
.remark-slide-content:after {
content: "Copyright Eli Lilly and Company";
position: absolute;
bottom: -5px;
left: 20px;
height: 40px;
width: 100%;
font-family: Helvetica, Arial, sans-serif;
font-size: 0.7em;
color: gray;
background-repeat: no-repeat;
background-size: contain;
}
</style>

```{r, include = FALSE}
options(
  warnPartialMatchArgs = FALSE,
  drake_clean_menu = FALSE,
  drake_make_menu = FALSE,
  htmltools.dir.version = FALSE
)
packages <- c(
  "drake",
  "keras",
  "recipes",
  "rsample",
  "tidyverse",
  "yardstick"
)
unlink(".RData")
purrr::walk(
  packages,
  function(pkg) {
    suppressMessages(suppressWarnings(library(pkg, character.only = TRUE)))
  }
)
clean(destroy = TRUE, verbose = FALSE)
unlink(".drake_history", recursive = TRUE, force = TRUE)
knitr::opts_chunk$set(
  collapse = TRUE,
  warning = FALSE,
  fig.align = "center",
  fig.width = 10,
  fig.height = 7,
  out.width = "80%",
  out.height = "80%"
)
```

## Large data science workflows

- Struggles

  1. Long runtimes.
  2. Many tasks.
  3. Interconnected tasks.

- Examples
  - Deep learning.
  - Classical machine learning.
  - Bayesian data analysis via Markov chain Monte Carlo.
  - Spatial data analysis.
  - Clinical trial modeling and simulation.
  - Subgroup identification.
  - Graph-based multiple comparison procedures.
  - Genomics pipelines.
  - PK/PD modeling.

???

Let's talk about what happens in large data science projects. Projects that are ambitious and complicated, that have a lot of moving parts and long runtimes. Projects that are difficult just to wrap your head around, that are difficult just to run from start to finish, but whose results really matter and carry important consequences.

---

## Interconnected tasks
<center>
<img src = "./images/workflow.png" align="middle" style="border: none; box-shadow: none; text-align: center;">
</center>

???

A large project has a bunch of interconnected tasks.

---

## When you change something...
<center>
<img src = "./images/change.png" align="middle" style="border: none; box-shadow: none; text-align: center;">
</center>

???

When you go back and change something...

---

## ...the downstream output is **no longer valid**.

<center>
<img src = "./images/downstream.png" align="middle" style="border: none; box-shadow: none; text-align: center;">
</center>

???

...everything that depends on it is no longer valid. And somehow you gotta pick up the pieces.

---

## Do you rerun **everything** from scratch?

- Not if you deal with long runtimes!

<center>
<img src = "./images/sisyphus.svg" align="middle" style="border: none; box-shadow: none; height: 375px; text-align: center;">
<div style="font-size: 0.5em; text-align: center"><a href="https://openclipart.org/detail/275842/sisyphus-overcoming-silhouette">https://openclipart.org/detail/275842/sisyphus-overcoming-silhouette</a></div>
</center>

???

For large projects, this is where the programming and workflow management techniques we are used to start to break down. If it takes several hours just to fit a single model, you're not going to want to rerun your whole analysis every time you make a change. It's just too much frustration to get stuck in a loop where you tweak something small and restart the code and tweak something small and restart the code, and pretty soon you're waiting forever.

This used to happen to me all the time. In my dissertation work, every model took about 3 hours, and I needed results from about 20 or 30 of them. I spent the last 6 or so months of grad school mostly restarting code and waiting it to finish. My results were totally out of sync, and it was hard to get any writing done under those conditions.

---

## Do you pick and choose what to update?

- Messy.
- Prone to human error.
- Not reproducible.

<center>
<img src = "./images/mess.svg" align="middle" style="border: none; box-shadow: none; height: 400px; text-align: center;">
<div style="font-size: 0.5em; text-align: center;"><a href="https://openclipart.org/detail/216179/messy-desk">https://openclipart.org/detail/216179/messy-desk</a></div>
</center>

???

But it's perilous to try to stop this cycle on your own without any help. If I'm the one deciding which code to skip and which code to rerun, it's a mess. It's too easy to make mistakes, and the results are not reproducible. I would not trust a human to do this.

---

## Solution: pipeline tools

<center>
<img src = "./images/infographic.svg" align="middle" style="border: none; box-shadow: none; text-align: center;">
</center>

- Tons exist already: [github.com/pditommaso/awesome-pipeline](https://github.com/pditommaso/awesome-pipeline).
- Most are language-agnostic or designed for Python or the shell.

???

Fortunately, we can *automate* the process of bringing results up to date as fast as possible. The standard solution is a pipeline tool. You may have heard of examples already. There's

- Make
- Snakemake
- Airflow
- Nextflow
- Luigi
- Dask

to name a few of the big names. But they're difficult to use with R. These kinds of tools generally try to be language-agnostic for the sake of versatility, but this means they add extra friction when you try to work with any one language in particular. My experience with Make specifically is that it obstructs my relationship with R. It pulls me out of the language and into sub-optimal programming practices.

---

## What distinguishes `drake`?

<center>
<img src = "./images/R.png" align="middle" style="border: none; box-shadow: none; text-align: center; height: 200px">
</center>

- Aggressively designed for R.
1. Think **functions**, not script files.
2. Think **variables**, not output files.
3. Think **data frames**, not `Makefile`s.
- [`drake`](https://github.com/ropensci/drake) borrows (1) and (2) from the [`remake`](https://github.com/richfitz/remake) package by [Rich FitzJohn](https://github.com/richfitz).
- [`remake`](https://github.com/richfitz/remake) is no longer under development.
- [`drake`](https://github.com/ropensci/drake) tries to extend [`remake`](https://github.com/richfitz/remake)'s ideas further and handle larger projects.

???


drake, on the other hand, is designed for R at its very core. It works entirely *within* the language, and it nudges you to write good code.

1. Instead of imperative script files, you write **functions**.

2. Instead of saving output files manually and dealing with all the headache of trying to organize them yourself, you return **objects** from those functions, and drake saves them for you automatically.

3. And instead of Makefiles in Make or YAML files in remake, you have an R-focused interface to define the skippable steps of your workflow.

By the way, drake was not the first pipeline tool to respect the way R works. Before drake, there was a package called `remake` by Rich FitzJohn. remake was playing nicely with objects and functions long before I created drake.

But remake isn't maintained anymore. And because it required you to work with YAML configuration files and supply your actual R scripts directly, it kind of had one foot in and one foot out anyway. drake tries to finish what remake started and scale up the intensity of work you can do with it.

---

## Example: a deep learning workflow

- Goal: predict customers who cancel their subscriptions with a telecom company.
- Data: [IBM Watson Telco Customer Churn dataset](https://www.ibm.com/communities/analytics/watson-analytics-blog/predictive-insights-in-the-telco-customer-churn-data-set/).
- Workflow principles generalize to other industries.

<img src = "./images/combine.png" style="border: none; box-shadow: none; height: 200px">

<div style="font-size: 0.5em;"><a href="https://openclipart.org/detail/90739/newplus">https://openclipart.org/detail/90739/newplus</a>, <a href="https://github.com/rstudio/keras">https://github.com/rstudio/keras</a></div>

???

To dive into drake, we're going to use a machine learning example. We've got a deep neural net, and we're going to use it to predict "customer churn", which is another way of saying attrition, or dropout.

---

background-image: ./images/not.png

## <img src="./images/no.png" width="40" height="40"> Let's move beyond numbered scripts.

```{r, eval = FALSE}
run_everything.R
R/
├── 01-data.R
├── 02-munge.R
├── 03-model.R
├── 04-results.R
└── 05-plot.R
data/
└── customer_churn.csv
output/
├── model_relu.h5
├── model_sigmoid.h5
├── confusion_matrix.rds
└── metrics_plot.png
```

???

But before we even begin to use `drake`, we need to change the way we write code. This project is too big for the classic "numbered script" paradigm that so many of us use. Whenever I try it, it explodes. And I'm sure I'm not the only one.

---

## <img src="./images/no.png" width="40" height="40"> Why not numbered scripts?

- The planning and the execution happen at the same time.
- Too cumbersome, ad hoc, and tangled for ambitious projects.

```{r, eval = FALSE}
# 02-munge.R
library(recipes) # Package dependencies scattered across scripts. #<<

rec <- data %>% # Single-use code, difficult to test. #<<
  training() %>%
  recipe(Churn ~ .) %>%
  step_rm(customerID) %>%
  step_naomit(all_outcomes(), all_predictors()) %>%
  step_discretize(tenure, options = list(cuts = 6)) %>%
  step_log(TotalCharges) %>%
  step_mutate(Churn = ifelse(Churn == "Yes", 1, 0)) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_center(all_predictors(), -all_outcomes()) %>%
  step_scale(all_predictors(), -all_outcomes()) %>%
  prep()

saveRDS(rec, "recipe.rds") # Final output scattered across code. #<<
```

???

I think the reason is that it tries to do too many things at once. The planning and the execution happen at the same time, and the actual content of the analysis is muddled with the bookkeeping.

In these scripts, you've got package dependencies and file-saving steps scattered all over the place. And for this preprocessing recipe, it looks like one thing, but it's really two things. You're defining a complicated procedure, and you're running it **as** you're defining it. We would do much better to define this thing in one place and call it somewhere else. Otherwise, it's hard to take the code out of context and test it.

But more importantly, we want to be able to step back and think about what we're doing. We need modularity, and we need structure around our ideas. We want to somewhow write the flow of our project in our own custom high-level shorthand that makes everything easier to read.

---

## <img src="./images/yes.png" width="60" height="40"> Instead, embrace **functions**!

- A function is a reusable command that accepts one or more inputs and returns a single output.
- It's a piece of custom shorthand for a single idea.

```{r}
add_things <- function(argument1, argument2) {
  argument1 + argument2
}

add_things(1, 2)

add_things(c(3, 4), c(5, 6))
```

???

The way we do that is with functions. A function is a reusable command that accepts one or more inputs and returns a single output. In practical terms, it's a piece of custom shorthand for a single idea. You define it once, you give it a name, and you call it whenever you need it on whatever data that fits. You can run it in production, or you can take it out of context to debug and test it.

---

## Why use functions?

1. Clarity: break down complicated ideas into manageable pieces.
2. Use R as intended.

    >    - Everything that exists is an object.
    >    - Everything that happens is a function call.
    >
    > John Chambers

3. Reuse: define once, run wherever.

???

But functions do so much more than that. They're not just for code you want to repeat and reuse. They're for code you want to **understand**.

Functions break down complicated ideas into manageable pieces. Instead of confronting the entirety of the project all at once, you deal with smaller components that are less intimidating and easier to work with.

As I said, functions are shorthand. They're also like paragraphs. The mental experience inside your head of writing functions is the same mental experience of writing paragraphs. We need *structure* in the text to fit the *structure* of our ideas so we can write stuff that makes sense. This is where functions succeed and R Markdown chunks tend to fall short. With functions, you're crystallizing a stream of consciousness into clear, discrete, explainable, composable, chainable points you want to make.

Now this may seem like extra work. But the activity of writing code is always a challenge. It always going to take focus, and it's always going to take effort. For **every single one** of us. And the more we accept that and lean into it, the better off we are in the long run.

Once you develop the habit, it's hard to go back because it fits what you're doing so well. Functions are true to the process of data analysis itself because they are **fundamentally designed** to describe change. They are **designed** to transform things into other things. And in our workflows, we change raw data into clean data, clean data into model output, and so on. Those steps are *begging* to be functions.

You're also being true to the R language. R was designed to be function-oriented. The more you express your thoughts as functions, the more you are using the language as it was **designed** to be used.

---

## Functions in a workflow

```{r, eval = FALSE}
make.R
R/
├── packages.R
├── functions.R #<<
└── plan.R
data/
└── customer_churn.csv
.drake/ # drake's cache
└──     # Output automatically appears here.
```

???

So let's use functions in the deep learning example. They're going to occupy most of the real estate in our scripts. This `functions.R` file usually gets pretty big, and I often split it up into multiple files.

---

## Functions in a workflow

```{r, eval = FALSE}
# packages.R: all package dependencies #<<
library(recipes)
# other packages...
````

```{r, eval = FALSE}
# functions.R: pure reusable code #<<
prepare_recipe <- function(data) {
  data %>%
    training() %>%
    recipe(Churn ~ .) %>%
    step_rm(customerID) %>%
    step_naomit(all_outcomes(), all_predictors()) %>%
    step_discretize(tenure, options = list(cuts = 6)) %>%
    step_log(TotalCharges) %>%
    step_mutate(Churn = ifelse(Churn == "Yes", 1, 0)) %>%
    step_dummy(all_nominal(), -all_outcomes()) %>%
    step_center(all_predictors(), -all_outcomes()) %>%
    step_scale(all_predictors(), -all_outcomes()) %>%
    prep()
}
# other functions...
```

???

Our preprocessing task is a single, clear, distinct idea with clearly-defined inputs and a single return value. So we put it in a function.

---

## Functions in a workflow

```{r}
# later in functions.R...

run_everything <- function() {
  data <- read_csv(file_in("data/customer_churn.csv"), col_types = cols()) %>%
    initial_split(prop = 0.3)
  saveRDS(data, "output/data.rds")

  rec <- prepare_recipe(data) # Call your other functions. #<<
  saveRDS(rec, "output/rec.rds")

  model_relu <- train_model(rec, act1 = "relu")
  save_model_hdf5(model_relu, "output/model_relu.h5")
  # more models...

  conf_sigmoid <- confusion_matrix(data, rec, model_sigmoid)
  saveRDS(conf_sigmoid, "output/conf_sigmoid.rds")
  # more confusion matrices...

  metrics <- compare_models(conf_relu, conf_sigmoid)
  saveRDS(metrics, "output/metrics.rds")
}
```

???

And we can take the functions we define and call them from other functions, including a function that runs everything at the top level and controls the overall flow of the analysis in our own custom high-level shorthand that makes everything easier to read.

---

## Conduct your analysis with your **functions**.

```{r, eval = FALSE}
# run_everything.R
source("R/packages.R")
source("R/functions.R")
run_everything()
```

???

Our top-level *script* does as little as possible. It pretty much just calls our "run everything" function.

Now this is a good time to step back. This whole approach that got us here, the appropriate use of functions, is always available to you, `drake` or no `drake`.

---

## But we can still do better...

- Avoid rerunning all the computation every time.
- Avoid micromanaging output files.

<center>
<img src = "./images/genie.png" align="middle" style="border: none; box-shadow: none; height: 375px; text-align: center;">
<div style="font-size: 0.5em; text-align: center"><a href="https://publicdomainvectors.org/en/free-clipart/Golden-magic-lamp/61683.html">https://publicdomainvectors.org/en/free-clipart/Golden-magic-lamp/61683.html</a></div>
</center>

???

But we can still do better. We can avoid rerunning all the computation every time. We can avoid micromanaging output files. All we need to do is cash in on all that attention and care we put into writing functions, and put them in `drake`.

---

## Enter drake! Define a **plan**.

```{r}
plan <- drake_plan(
  rec = prepare_recipe(data), # Use your functions. #<<
  model = target(
    train_model(rec, act1 = act),
    format = "keras",
    transform = map(act = c("relu", "sigmoid"))
  ),
  conf = target(
    confusion_matrix(data, rec, model),
    transform = map(model, .id = act)
  ),
  metrics = target(
    compare_models(conf),
    transform = combine(conf)
  ),
  data = read_csv(                      # flexible target order, #<<
    file_in("data/customer_churn.csv"), # flexible commands #<<
    col_types = cols()
  ) %>%
    initial_split(prop = 0.3)
)
```

???

Instead of a `run_everything()` function, we create a drake plan. The plan defines a bunch of skippable steps. Each step has a command, and each command calls the R functions it needs, and returns a value called a **target**. We give these targets the names you see on the left, and we list them in any order we want. So we can even define the data at the bottom here.

---

## The plan is a data frame of skippable tasks.

```{r}
plan
```

???

This plan is a tidy data frame of commands and targets. This is what you get instead of a Makefile. It exists entirely within R, and it uses a data structure we all like.

---

## The workflow

<br>
<center>
<img align="middle" src = "./images/vis1.png" style="border: none; box-shadow: none;">
</center>

???

Now as a first step, `drake` takes all this in, and it analyzes your plan, and it analyzes your functions, without running any of your code just yet. It detects the symbols that you mention, and any optional input files you choose to declare, and it decides which steps go first and which steps go last. So it puts "rec" after "data" because the command for "rec" mentions the symbol "data" literally in the code. And similarly, it puts together an entire flowchart of tasks, an entire directed acyclic *graph* of tasks.

---

## Run the project in make.R.

```{r}
# like run_everything.R...
source("R/packages.R")
source("R/functions.R")
source("R/plan.R")

make(plan)
```

???

So after you source your packages and functions into your session, the `make()` function does all that setup and code analysis I described, it constructs the graph, and it runs the correct commands in the correct order on that graph, and it stores the targets in a hidden cache.

---

## Compare models.

```{r}
readd(metrics) # See also loadd()
```

???

After that, you can retrieve any target at any time with convenient interface functions `loadd()` and `readd()`.

---

## Add a new model.

```{r}
plan <- drake_plan(
  rec = prepare_recipe(data),
  model = target(
    train_model(rec, act1 = act),
    format = "keras",
    transform = map(act = c("relu", "sigmoid", "softmax")) #<<
  ),
  conf = target(
    confusion_matrix(data, rec, model),
    transform = map(model, .id = act)
  ),
  metrics = target(
    compare_models(conf),
    transform = combine(conf)
  ),
  data = read_csv(
    file_in("data/customer_churn.csv"),
    col_types = cols()
  ) %>%
    initial_split(prop = 0.3)
)
```

```{r, echo = FALSE}
tmp <- file.copy("R/plan.R", "R/tmp.R", overwrite = TRUE)
tmp <- file.copy("R/plan2.R", "R/plan.R", overwrite = TRUE)
```

???

After you explore a bit, then it comes time for changes. 
For this example, we go back and add another model and another confusion matrix to try to outperform the other models. We could have gone back and changed our commands and functions too.

---

## vis_drake_graph()

<center>
<img align="middle" src = "./images/vis2.png" style="border: none; box-shadow: none;">
</center>

???

But whatever we do, drake looks at all of that and decides which targets are up to date and which are outdated or missing. And there are several interface functions to display this information in different ways.

Here, since we added a new model, that model and everything downstream are out of date. But the green targets are still up to date because we haven't changed any **functions** or files or commands or upstream targets or random number generator seeds they depend on.

---

## Refresh the results in make.R.

```{r}
source("R/packages.R")
source("R/functions.R")
source("R/plan.R") # modified #<<

make(plan)
```

???

When we run the new plan, we only build the outdated or missing targets. We run only **part** of the work, not all of it. Small changes to code or data no longer necessarily invalidate everything. We save a ton of time this way.

In computing, we like to talk a lot about speed. How do we make our code run faster? How do we make it more efficient? But the fastest code is the code we don't run at all.

---

## Compare models.

```{r}
readd(metrics)
```

???

And with minimal effort, our metrics are now up to date. The plot has all three models now.

---

## Evidence of reproducibility

```{r}
source("R/packages.R")
source("R/functions.R")
source("R/plan.R")

make(plan)
```

- See also `outdated()`.

???

If we run the `make()` function again without having changed anything, `drake` tells us that everything is up to date and does nothing else.

This is *tangible evidence* that your output matches the code and data it came from. It's a stamp of validation that you can recompute quickly at any time. It is *tangible evidence* of reproducibility because it increases our ability to trust our results. As Gabe Becker likes to say, reproducibility boils down to trust.

---

## Efficient data formats

- Increased speed and reduced memory consumption.

```{r, eval = FALSE}
library(drake)
n <- 1e8 # Each target is 1.6 GB in memory.
plan <- drake_plan(
  data_fst = target(
    data.frame(x = runif(n), y = runif(n)),
    format = "fst" #<<
  ),
  data_old = data.frame(x = runif(n), y = runif(n))
)
make(plan)
#> target data_fst
#> target data_old
build_times(type = "build")
#> # A tibble: 2 x 4
#>   target   elapsed              user                 system    
#>   <chr>    <Duration>           <Duration>           <Duration>
#> 1 data_fst 13.93s               37.562s              7.954s #<<
#> 2 data_old 184s (~3.07 minutes) 177s (~2.95 minutes) 4.157s #<<
```

???

Now that's where I usually stop. But since we have time, let's talk about some new features.

drake now has special formats for more efficient data processing: one for data frames, one for data tables, one for Keras models, and an alternative RDS format that uses less memory than drake's default caching.

The idea is that if we know we have a data frame or we know we have a Keras model, we can save and retrieve it a lot faster. I'm looking to add formats and increase efficiency in general, so please let me know if you have ideas.

---

## History and provenance

```{r}
drake_history()
```

???

`drake` also tracks history and provenance. Up to date or not, you can see what you built, when you built it, how much time it took, and the top-level function arguments you used in your commands. Best of all, you can see multiple versions of multiple targets across the whole timeline of your project.

---

## Reproducible data recovery

```{r}
clean() # Oops!

start <- proc.time()
make(plan, recover = TRUE)

proc.time() - start
```

- Details + how to rename a target: <https://ropenscilabs.github.io/drake-manual/walkthrough.html#reproducible-data-recovery-and-renaming>

???

And depending on the conditions under which those old targets were built, you can recover them instead of building them all over again. Sometimes, the command, the upstream targets, the functions, the files, and the random number generator seed are the same as for some run from the past. And in those cases, you can save extra time with data recovery.

Unfortunately, this is not the default because it assumes you are always using the same packages and computing environment. `drake` doesn't try to get involved with those things. To lock down your package dependencies and all that, `renv` and Docker are great tools. Not only are they fully compatible with `drake`, they help compensate for its relative weaknesses.

---

## Dependency-aware high-performance computing

- Just a little configuration...

```{r, eval = FALSE}
# template file with configuration
drake_hpc_template_file("slurm_clustermq.tmpl")

# Use SLURM resource manager with the template.
options(
  clustermq.scheduler = "slurm",
  clustermq.template = "slurm_clustermq.tmpl"
)

# make() is the basically the same.
make(plan, jobs = 2, parallelism = "clustermq")
```

???

And here's another feature that's been around for a long time: parallel computing. drake automatically decides which targets can run at the same time and which need to wait for dependencies. How it works is you declare the number of workers, the parallel backend, and resource requirements if you're on a cluster...

---

## Dependency-aware high-performance computing

<iframe width="800" height="450" src="https://www.powtoon.com/embed/bUfSIaXjrw5/" frameborder="0"></iframe>

???

And drake takes care of the rest. It launches a bunch of workers and sends those workers to the targets as soon as they're ready to go. It automatically accounts for the fact that some targets depend on others. 

---

## Resources

- Get [`drake`](https://github.com/ropensci/drake):

```{r, eval = FALSE}
install.packages("drake")
```

- Example code from these slides:

```{r, eval = FALSE}
drake::drake_example("customer-churn")
```

- Workshop materials:

```{r, eval = FALSE}
remotes::install_github("wlandau/learndrake")
```

???

Here's how to learn more about drake. Just as drake itself exists and operates entirely within R, so do many of its resources.

---

## Links

- Development repository: <https://github.com/ropensci/drake>
- Full user manual <https://ropenscilabs.github.io/drake-manual>
- Reference website: <https://docs.ropensci.org/drake>
- Hands-on workshop: <https://github.com/wlandau/learndrake>
- Code examples: <https://github.com/wlandau/drake-examples>
- Discuss at rOpenSci.org: <https://discuss.ropensci.org>

## rOpenSci use cases

- Use [`drake`](https://github.com/ropensci/drake)? Share your use case at <https://ropensci.org/usecases>.

<center>
<img src = "./images/ropensci.png" style="border: none; box-shadow: none; height: 150px">
</center>

???

For the online stuff, I highly recommend the "learndrake" workshop. Just go to the GitHub page and click the blue "launch binder" badge in the README, and suddenly you're in RStudio Server with everything ready to go. Karthik's `holepunch` package made setting that up really smooth.

And if you use `drake` for a real project, please consider writing about it as an rOpenSci use case. It enhances these resources, it demonstrates the value of rOpenSci packages, and it grows the community.

Ben Bond-Lamberty already wrote this one. Many more example projects are emerging, some of which are listed here in the manual. I still need to add more, including the great work Pat Schratz is doing to integrate `drake` with `mlr3`.

---

## Thanks

<br>
<br>
<table style = "border: none">
<tr>
<td style = "padding-right: 125px">
<ul style>
<img src = "./images/edgar.jpg" style="border: none; box-shadow: none; height: 150px">
<li><a href = "https://github.com/edgararuiz">Edgar Ruiz</a></li>
<li><a href = "https://github.com/sol-eng/tensorflow-w-r/blob/master/workflow/tensorflow-drake.Rmd">example code</a></li>
</ul>
</td>
<td>
<ul>
<img src = "./images/matt.jpg" style="border: none; box-shadow: none; height: 150px">
<li><a href = "https://github.com/mdancho84">Matt Dancho</a></li>
<li><a href = "https://blogs.rstudio.com/tensorflow/posts/2018-01-11-keras-customer-churn/">blog post</a></li>
</ul>
</td>
</tr>
</table>

???

Today, the deep learning example came from Matt Dancho and Edgar Ruiz.

---

## Thanks

<table style = "border: none">
<tr>
<td>
<br>
<ul>
<img src = "./images/ropensci.png" style="border: none; box-shadow: none; height: 150px">
<li><a href = "https://github.com/maelle">Maëlle Salmon</a></li>
<li><a href = "https://github.com/benmarwick">Ben Marwick</a></li>
<li><a href = "https://github.com/jules32">Julia Lowndes</a></li>
<li><a href = "https://github.com/gothub">Peter Slaughter</a></li>
<li><a href = "https://github.com/jennybc">Jenny Bryan</a></li>
<li><a href = "https://github.com/richfitz">Rich FitzJohn</a></li>
<li><a href = "https://github.com/stefaniebutland">Stefanie Butland</a></li>
</ul>
</td>
<td>
<ul>
<li><a href = "https://github.com/jarad">Jarad Niemi</a></li>
<li><a href = "https://github.com/krlmlr">Kirill Müller</a></li>
<li><a href = "https://github.com/HenrikBengtsson">Henrik Bengtsson</a></li>
<li><a href = "https://github.com/mschubert">Michael Schubert</a></li>
<li><a href = "https://github.com/kendonB">Kendon Bell</a></li>
<li><a href = "https://github.com/milesmcbain">Miles McBain</a></li>
<li><a href = "https://github.com/pat-s">Patrick Schratz</a></li>
<li><a href = "https://github.com/AlexAxthelm">Alex Axthelm</a></li>
<li><a href = "https://github.com/dapperjapper">Jasper Clarkberg</a></li>
<li><a href = "https://github.com/tiernanmartin">Tiernan Martin</a></li>
<li><a href = "https://github.com/BListyg">Ben Listyg</a></li>
<li><a href = "https://github.com/tjmahr">TJ Mahr</a></li>
<li><a href = "https://github.com/bpbond">Ben Bond-Lamberty</a></li>
<li><a href = "https://github.com/tmastny">Tim Mastny</a></li>
<li><a href = "https://github.com/billdenney">Bill Denney</a></li>
<li><a href = "https://github.com/aedobbyn">Amanda Dobbyn</a></li>
<li><a href = "https://github.com/dfalster">Daniel Falster</a></li>
<li><a href = "https://github.com/rkrug">Rainer Krug</a></li>
<li><a href = "https://github.com/bmchorse">Brianna McHorse</a></li>
<li><a href = "https://github.com/mrchypark">Chan-Yub Park</a></li>
</ul>
</td>
</tr>
</table>

???

For the development of drake itself, I have a lot of people to thank, many more than are on this list.

rOpenSci in particular is the one of the friendliest and most welcoming communities in tech, an amazing combination of expertise and approachability. `drake` is a better package and I am a better developer because they coached me during the software review process, they got the word out, and they connected me to users with insightful ideas and developers who I look up to.

Also, I learned a lot about how other people see workflows from Jenny Bryan, Hilary Parker, Ellis Hughes, Pat Schratz, Stas Kolenikov, and Michael Lawrence. I used to take functions for granted, but thanks to them, I am making more of a point to explain the style of coding that `drake` requires.

---

## A riddle!

- From a math PhD oral exam:

> Define an example of a nontrivial function.

- Hint: the best answers do not even come from math or computing!

<center>
<img src = "./images/pythagoras.png" align="middle" style="border: none; box-shadow: none; height: 375px; text-align: center;">
<div style="font-size: 0.5em; text-align: center"><a href="https://publicdomainvectors.org/en/free-clipart/Pythagoras-tree/58775.html">https://publicdomainvectors.org/en/free-clipart/Pythagoras-tree/58775.html</a></div>
</center>

???

Let's end the talk portion with a riddle. Try to think of an example of a highly nontrivial function.

I heard this was originally part of a math PhD oral exam, but all you need to know is the definition of a function. Apparently, the best answers are unrelated to math or computing or statistics or data science. I do have an answer in mind, but I'm not going to spoil it just yet.

I recommend taking something you already think is nontrivial, no matter what it is in your life, and try turning it into a function. I'm curious what people come up with, and I think it could be a good exercise to get used to writing functions more often.

```{r, include = FALSE}
tmp <- file.copy("R/plan.R", "R/plan2.R", overwrite = TRUE)
tmp <- file.copy("R/tmp.R", "R/plan.R", overwrite = TRUE)
unlink("R/tmp.R")
clean(destroy = TRUE)
unlink(".drake_history", recursive = TRUE, force = TRUE)
```
