---
title: "Reproducible workflows at scale with drake"
author: Will Landau
output: 
  xaringan::moon_reader:
    nature:
      highlightLines: true
---

<style>
.inverse {
background-color: transparent;
text-shadow: 0 0 0px transparent;
}
.title-slide {
vertical-align: bottom !important; 
text-align: center !important;
}
.title-slide h1 {
position: absolute;
top: 0;
left: 0;
right: 0;
width: 100%;
line-height: 4em;
color: #666666;
}
.title-slide h3 {
line-height: 6em;
color: #666666;
}
.title-slide {
background-color: white;
background-image: url('images/logo.png');
background-repeat: no-repeat;
background-size: 25%;
}
.remark-slide-content:after {
content: "Copyright Eli Lilly and Company";
position: absolute;
bottom: -5px;
left: 20px;
height: 40px;
width: 100%;
font-family: Helvetica, Arial, sans-serif;
color: gray;
background-repeat: no-repeat;
background-size: contain;
}
</style>

```{r c1, include = FALSE}
options(
  warnPartialMatchArgs = FALSE,
  drake_clean_menu = FALSE,
  drake_make_menu = FALSE,
  htmltools.dir.version = FALSE
)
packages <- c(
  "drake",
  "keras",
  "recipes",
  "rsample",
  "tidyverse",
  "yardstick"
)
unlink(".RData")
purrr::walk(
  packages,
  function(pkg) {
    suppressMessages(suppressWarnings(library(pkg, character.only = TRUE)))
  }
)
clean(destroy = TRUE, verbose = FALSE)
unlink(".drake_history", recursive = TRUE, force = TRUE)
knitr::opts_chunk$set(
  collapse = TRUE,
  warning = FALSE,
  fig.align = "center",
  fig.width = 10,
  fig.height = 7,
  out.width = "80%",
  out.height = "80%"
)
```

## Large data science workflows

- Struggles

  1. Long runtimes.
  2. Many tasks.
  3. Interconnected tasks.

- Examples
  - Deep learning.
  - Classical machine learning.
  - Bayesian data analysis via Markov chain Monte Carlo.
  - Spatial data analysis.
  - Clinical trial modeling and simulation.
  - Subgroup identification.
  - Graph-based multiple comparison procedures.
  - Genomics pipelines.
  - PK/PD modeling.

???

Let's talk about what happens in large data science projects. Projects that are ambitious and complicated, that have a lot of moving parts and long runtimes. Projects that are difficult just to wrap your head around, that are difficult to run from start to finish, and that have important consequences in business decisions and academic research.

---

## Interconnected tasks
<center>
<img src = "./images/workflow.png" align="middle" style="border: none; box-shadow: none; text-align: center;">
</center>

???

The tasks of a data analysis project are interconnected.

---

## When you change something...
<center>
<img src = "./images/change.png" align="middle" style="border: none; box-shadow: none; text-align: center;">
</center>

???

When you go back and change something...

---

## ...the downstream output is **no longer valid**.

<center>
<img src = "./images/downstream.png" align="middle" style="border: none; box-shadow: none; text-align: center;">
</center>

???

...everything that depends on it is no longer valid. And you have a lot of updating to do.

---

## Do you rerun **everything** from scratch?

- Not if you deal with long runtimes!

<center>
<img src = "./images/sisyphus.svg" align="middle" style="border: none; box-shadow: none; height: 375px; text-align: center;">
<div style="font-size: 0.5em; text-align: center"><a href="https://openclipart.org/detail/275842/sisyphus-overcoming-silhouette">https://openclipart.org/detail/275842/sisyphus-overcoming-silhouette</a></div>
</center>

???

For large projects, this is where the programming and workflow management techniques we are used to start to break down. If it takes several hours just to fit a single model, you're not going to want to rerun your whole analysis every time you make a change. It's just too much frustration.

But I used to do this all the time. In my dissertation work, every model took about 4 hours, and I needed results from about 20 or 30 of them. I spent the last 6 months of grad school mostly waiting for code to finish running. It was hard to get any thesis writing done without fully up-to-date results.

---

## Do you pick and choose what to update?

- Messy.
- Prone to human error.
- Not reproducible.

<center>
<img src = "./images/mess.svg" align="middle" style="border: none; box-shadow: none; height: 400px; text-align: center;">
<div style="font-size: 0.5em; text-align: center;"><a href="https://openclipart.org/detail/216179/messy-desk">https://openclipart.org/detail/216179/messy-desk</a></div>
</center>

???

And it's perilous to try to stop this cycle on your own without any help. If I'm the one deciding which code to skip and which code to rerun, it's a mess. It's too easy to make big mistakes, and the results are not reproducible. I would not trust a human to do this.

---

## Solution: pipeline tools

<center>
<img src = "./images/infographic.svg" align="middle" style="border: none; box-shadow: none; text-align: center;">
</center>

- Tons exist already: [github.com/pditommaso/awesome-pipeline](https://github.com/pditommaso/awesome-pipeline).
- Most are language-agnostic or designed for Python or the shell.

???

Fortunately, we can *automate* the process of bringing results up to date as fast as possible. The solution is a pipeline tool. You may have heard of examples already. There's

- Make
- Snakemake
- Airflow
- Nextflow
- Luigi
- Dask

But they're difficult to use with R. They try to be language-agnostic for the sake of versatility, but this means they add extra friction when you try to work with any one language in particular. My experience with Make specifically is that it obstructs my relationship with R. It pulls me out of the language and into sub-optimal programming practices.

---

## What distinguishes `drake`?

<center>
<img src = "./images/R.png" align="middle" style="border: none; box-shadow: none; text-align: center; height: 200px">
</center>

- Aggressively designed for R.
1. Think **functions**, not script files.
2. Think **variables**, not output files.
3. Think **data frames**, not `Makefile`s.
- [`drake`](https://github.com/ropensci/drake) borrows (1) and (2) from the [`remake`](https://github.com/richfitz/remake) package by [Rich FitzJohn](https://github.com/richfitz).
- [`remake`](https://github.com/richfitz/remake) is no longer under development.
- [`drake`](https://github.com/ropensci/drake) tries to extend [`remake`](https://github.com/richfitz/remake)'s ideas further and handle larger projects.

???


drake, on the other hand, is designed for R at its very core. It works entirely *within* the language, and it nudges you to write good code.

1. Instead of imperative script files, you write **functions**.

2. Instead of saving output files manually and dealing with all the headache of trying to organize them yourself, you return **objects** from those functions, and drake saves them for you.

3. And instead of Makefiles in Make or YAML files in remake, you have an R-focused interface to define the skippable steps of your workflow.

By the way, drake was not the first pipeline tool to truly respect R. Before drake, there was a package called `remake` by Rich FitzJohn. remake was playing nicely with objects and functions long before I created drake.

But remake is no longer maintained. And because it required you to work with YAML configuration files and because of some other restrictions on how you write your code, it kind of had one foot in R and one foot out anyway. drake tries to finish what remake started and scale up the intensity of work you can do with it.

---

## Example: a deep learning workflow

- Goal: predict customers who cancel their subscriptions with a telecom company.
- Data: [IBM Watson Telco Customer Churn dataset](https://www.ibm.com/communities/analytics/watson-analytics-blog/predictive-insights-in-the-telco-customer-churn-data-set/).
- Workflow principles generalize to other industries.

<img src = "./images/combine.png" style="border: none; box-shadow: none; height: 200px">

<div style="font-size: 0.5em;"><a href="https://openclipart.org/detail/90739/newplus">https://openclipart.org/detail/90739/newplus</a>, <a href="https://github.com/rstudio/keras">https://github.com/rstudio/keras</a></div>

???

To dive into drake, we're going to use a machine learning example. We've got a deep neural net, and we're going to use it to predict "customer churn", which is another way of saying attrition, or dropout.

---

background-image: ./images/not.png

## <img src="./images/no.png" width="40" height="40"> Let's move beyond numbered scripts.

```{r files_bad, eval = FALSE}
run_everything.R
R/
├── 01-data.R
├── 02-munge.R
├── 03-model.R
├── 04-results.R
└── 05-plot.R
data/
└── customer_churn.csv
output/
├── model_relu.h5
├── model_sigmoid.h5
├── confusion_matrix.rds
└── metrics_plot.png
```

???

Projects like this tend to get too big for the classic "numbered script" paradigm you may have seen. In my experience, it scales poorly for large projects.

---

## <img src="./images/no.png" width="40" height="40"> Why not numbered scripts?

- The planning and the execution happen at the same time.
- Too cumbersome, ad hoc, and tangled for ambitious projects.

```{r c2, eval = FALSE}
# 02-munge.R
library(recipes) # Package dependencies scattered across scripts. #<<

rec <- data %>% # Single-use code, difficult to test. #<<
  training() %>%
  recipe(Churn ~ .) %>%
  step_rm(customerID) %>%
  step_naomit(all_outcomes(), all_predictors()) %>%
  step_discretize(tenure, options = list(cuts = 6)) %>%
  step_log(TotalCharges) %>%
  step_mutate(Churn = ifelse(Churn == "Yes", 1, 0)) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_center(all_predictors(), -all_outcomes()) %>%
  step_scale(all_predictors(), -all_outcomes()) %>%
  prep()

saveRDS(rec, "recipe.rds") # Final output scattered across code. #<<
```

???

And the reason is that it tries to do too many things at once. The planning and the execution happen at the same time, and the actual content of the analysis is muddled with the bookkeeping.

Look at these scripts. You've got package dependencies scattered everywhere, and file-saving steps scattered everywhere. And look at this preprocessing recipe. It looks like one thing, but it's really two different things. You're defining a complicated procedure, and you're running it **as** you're defining it. These things should really be separate. Otherwise, it's hard to take the code out of context and test it.

Not only that, we don't have shorthand names for the long complicated steps we're doing. We don't have the structure to organize the ideas, so it's hard to take a step back and think about what we're doing.

In the R community, we like to criticize Microsoft Excel. We give it a hard time for putting the formulas and the spreadsheets in the same file, and we claim it's not reproducible. And we have a point. But we are guilty of something very similar! We're knitting and weaving and tangling too many things together!

---

## <img src="./images/yes.png" width="60" height="40"> Instead, embrace **functions**!

- A function is a reusable command that accepts one or more inputs and returns a single output.

```{r def}
my_function <- function(argument1, argument2) {
  argument1 + argument2
}

my_function(1, 2)

my_function(3, 4)
```

???

We can do better. In computing, the easiest, most ubiquitous, most standard way to solving this problem is to write functions.

A function is just a reusable command that accepts one or more inputs and returns a single output. You define it once, and you call it whenever you need it on whatever data that fits.

---

## Why use functions?

1. Clarity: break down complicated ideas into manageable pieces.
2. Use R as intended.

    >    - Everything that exists is an object.
    >    - Everything that happens is a function call.
    >
    > John Chambers

3. Reuse: define once, run wherever.

???

But functions do so much more than that. They're not just for code you want to repeat and reuse. They're for code you want to **understand**.

Functions break down complicated ideas into manageable pieces. Instead of a monolithic mess, you have components that are smaller, cleaner, and less intimidating.

Functions are like paragraphs of a data analysis. It's the same mental experience inside your head. You're crystallizing a stream of consciousness into clear, discrete, explainable points you want to make.

Once you start doing this, you'll also notice that you're being true to the process of data analysis itself. Functions are a universal tool to describe how things change into other things. And that's exactly what we do when we analyze data. We change raw data into clean data, clean data into model output, and so on.

You're also being true to the R language. You're using the language as it's **designed** to be used. The mindset of "everything that exists is an object, everything that happens is a function call" is one of the truest descriptions of a function-oriented language like R. It's what R **intends** to do, and you're going with the flow.

And yes, functions are practical too because you can use them repeatedly, maybe take them out of context and test them. But even so, most of the value is still conceptual.

---

## Functions in a workflow

```{r files_good, eval = FALSE}
make.R
R/
├── packages.R
├── functions.R #<<
└── plan.R
data/
└── customer_churn.csv
.drake/ # drake's cache
└──     # Output automatically appears here.
```

???

So let's use functions in the deep learning example. We can still have scripts, but they are better organized. We separate out the packages, the functions, and the plan (which I will get to later).

---

## Functions in a workflow

```{r c3.0, eval = FALSE}
# packages.R: all package dependencies #<<
library(recipes)
# other packages...
````

```{r c3, eval = FALSE}
# functions.R: pure reusable code #<<
prepare_recipe <- function(data) {
  data %>%
    training() %>%
    recipe(Churn ~ .) %>%
    step_rm(customerID) %>%
    step_naomit(all_outcomes(), all_predictors()) %>%
    step_discretize(tenure, options = list(cuts = 6)) %>%
    step_log(TotalCharges) %>%
    step_mutate(Churn = ifelse(Churn == "Yes", 1, 0)) %>%
    step_dummy(all_nominal(), -all_outcomes()) %>%
    step_center(all_predictors(), -all_outcomes()) %>%
    step_scale(all_predictors(), -all_outcomes()) %>%
    prep()
}
# other functions...
```

???

Our preprocessing task is a single, clear, distinct idea with well-defined inputs and a single return value. So we put it in a function. That function is builds on top of other functions we borrow from the `recipes` package.

---

## Functions in a workflow

```{r run_everything}
# later in functions.R...

run_everything <- function() {
  data <- read_csv(file_in("data/customer_churn.csv"), col_types = cols()) %>%
    initial_split(prop = 0.3)
  saveRDS(data, "output/data.rds")

  rec <- prepare_recipe(data) # Call your other functions. #<<
  saveRDS(rec, "output/rec.rds")

  model_relu <- train_model(rec, act1 = "relu")
  save_model_hdf5(model_relu, "output/model_relu.h5")
  # more models...

  conf_sigmoid <- confusion_matrix(data, rec, model_sigmoid)
  saveRDS(conf_sigmoid, "output/conf_sigmoid.rds")
  # more confusion matrices...

  metrics <- compare_models(conf_relu, conf_sigmoid)
  saveRDS(metrics, "output/metrics.rds")
}
```

???

Once we define a bunch of other functions, we group them into higher and higher-level functions until we have a single flagship function at the very top that runs everything. This gives us a lot of control over the flow of the analysis.

---

## Conduct your analysis with your **functions**.

```{r c3.5, eval = FALSE}
# run_everything.R
source("R/packages.R")
source("R/functions.R")
run_everything()
```

???

With our functions defined, our top-level **script** does as little as possible. It just loads the dependencies and calls a function. And that's a huge improvement.

---

## But we can still do better...

- Avoid rerunning every function on every runthrough.
- Avoid micromanaging output files.

<center>
<img src = "./images/genie.png" align="middle" style="border: none; box-shadow: none; height: 375px; text-align: center;">
<div style="font-size: 0.5em; text-align: center"><a href="https://publicdomainvectors.org/en/free-clipart/Golden-magic-lamp/61683.html">https://publicdomainvectors.org/en/free-clipart/Golden-magic-lamp/61683.html</a></div>
</center>

???

But we can still do better. We can avoid rerunning everything indiscriminately. We can avoid micromanaging output files. And that's where drake really helps.

---

## Enter drake! Define a **plan**.

```{r c4}
plan <- drake_plan(
  rec = prepare_recipe(data), # Use your functions. #<<
  model = target(
    train_model(rec, act1 = act),
    format = "keras",
    transform = map(act = c("relu", "sigmoid"))
  ),
  conf = target(
    confusion_matrix(data, rec, model),
    transform = map(model, .id = act)
  ),
  metrics = target(
    compare_models(conf),
    transform = combine(conf)
  ),
  data = read_csv(                      # flexible target order, #<<
    file_in("data/customer_churn.csv"), # flexible commands #<<
    col_types = cols()
  ) %>%
    initial_split(prop = 0.3)
)
```

???

Instead of a `run_everything()` function, we create a drake plan. The plan defines a bunch of skippable steps. Each step has a command, and each command returns a value called a **target**. We give these targets names on the left, and we list them in any order we want. So we can even define the data at the bottom here.

---

## The plan is a data frame of skippable tasks.

```{r c5}
plan
```

???

This plan is a tidy data frame of commands and targets. This is what you get instead of a Makefile. It exists entirely within R, and it uses a data structure we all like.

---

## The workflow

<br>
<center>
<img align="middle" src = "./images/vis1.png" style="border: none; box-shadow: none;">
</center>

???

Now `drake` takes this, and it analyzes your plan, and it analyzes your functions, without running any of your code. It detects the symbols that you mention and any optional input files you choose to declare, and it decides which steps go first and which steps go last. Because it analyzes your code, drake can figure this out regardless of the order you wrote the targets in the plan. For example, the "rec" target comes after the data because the command for "rec" mentions the symbol "data".

---

## Run the project in make.R.

```{r c6}
# like run_everything.R...
source("R/packages.R")
source("R/functions.R")
source("R/plan.R")

make(plan)
```

???

With all that in place, we can actually run the workflow now. We source the scripts to load our packages, functions, and plan, and we call the `make()` function to actually execute the tasks. The `make()` function runs the correct commands in the correct order, and it stores the targets in a hidden cache.

---

## Compare models.

```{r c7}
readd(metrics) # See also loadd()
```

???

After that, you can retrieve any target at any time with convenient interface functions `loadd()` and `readd()`.

---

## Add a new model.

```{r c8}
plan <- drake_plan(
  rec = prepare_recipe(data),
  model = target(
    train_model(rec, act1 = act),
    format = "keras",
    transform = map(act = c("relu", "sigmoid", "softmax")) #<<
  ),
  conf = target(
    confusion_matrix(data, rec, model),
    transform = map(model, .id = act)
  ),
  metrics = target(
    compare_models(conf),
    transform = combine(conf)
  ),
  data = read_csv(
    file_in("data/customer_churn.csv"),
    col_types = cols()
  ) %>%
    initial_split(prop = 0.3)
)
```

```{r c8write, echo = FALSE}
tmp <- file.copy("R/plan.R", "R/tmp.R", overwrite = TRUE)
tmp <- file.copy("R/plan2.R", "R/plan.R", overwrite = TRUE)
```

???

drake is designed for changes. Here, we go back and add another model and another confusion matrix to try to outperform the others. We could have gone back and changed our commands and functions too.

---

## vis_drake_graph()

<center>
<img align="middle" src = "./images/vis2.png" style="border: none; box-shadow: none;">
</center>

???

But whatever we do, drake looks at all of that and decides which targets are up to date and which are outdated or missing. And there are several interface functions to display this information in different ways.

Here, since we added a new model, that model and everything downstream are out of date. But the green targets are still up to date because we haven't changed any **functions** or files commands or upstream targets or random number generator seeds they depend on.

---

## Refresh the results in make.R.

```{r c9}
source("R/packages.R")
source("R/functions.R")
source("R/plan.R") # modified #<<

make(plan)
```

???

When we run the new plan, we only build the outdated or missing targets. We run only **part** of the work, not all of it. Small changes to code or data no longer always invalidate everything. We save a ton of time this way.

In computing, we like to talk a lot about speed. How do we make our code run faster? How do we make it more efficient? But the fastest code is the code we don't run at all.

---

## Compare models.

```{r c7updated}
readd(metrics)
```

???

And with minimal effort, our metrics are now up to date. The plot has all three models now.

---

## Evidence of reproducibility

```{r c10}
source("R/packages.R")
source("R/functions.R")
source("R/plan.R")

make(plan)
```

- See also `outdated()`.

???

If we run the `make()` function again without having changed anything, `drake` tells us that everything is up to date and does nothing else.

This is *tangible evidence* that your output matches the code and data it came from. It is *tangible evidence* of reproducibility.

Reproducibility can mean a lot of different things.

- Replicability at the lab bench when the data is generated.
- The amount of data you disclose to other researchers.
- How well you explain your methods.
- How often your method has been replicated in the literature.
- How credentialed the authors are themselves.

But as Gabe Becker likes to say, it all comes down to trust. Reproducibility is about trust. And if you're dealing with long runtimes, this trust is difficult to earn. drake enhances the kind of validation we need.

---

## Efficient data formats

- Increased speed and reduced memory consumption.

```{r, eval = FALSE}
library(drake)
n <- 1e8 # Each target is 1.6 GB in memory.
plan <- drake_plan(
  data_fst = target(
    data.frame(x = runif(n), y = runif(n)),
    format = "fst" #<<
  ),
  data_old = data.frame(x = runif(n), y = runif(n))
)
make(plan)
#> target data_fst
#> target data_old
build_times(type = "build")
#> # A tibble: 2 x 4
#>   target   elapsed              user                 system    
#>   <chr>    <Duration>           <Duration>           <Duration>
#> 1 data_fst 13.93s               37.562s              7.954s #<<
#> 2 data_old 184s (~3.07 minutes) 177s (~2.95 minutes) 4.157s #<<
```

???

Now for some new features.

drake now has special formats for more efficient data processing. There are currently two fst-based formats, one for data frames and other for data *table*s. There's also a special format for Keras models.

I'm always on the lookout for ways to improve efficiency, so please chime in if you have ideas.

---

## History and provenance

```{r history}
drake_history()
```

???

`drake` also tracks history and provenance. You can see all the versions of old targets you built in the past, when you built them, how long they took, and even which function arguments you used in your commands.

---

## Reproducible data recovery

```{r recovery}
clean() # Oops!

start <- proc.time()
make(plan, recover = TRUE)

proc.time() - start
```

- Details + how to rename a target: <https://ropenscilabs.github.io/drake-manual/walkthrough.html#reproducible-data-recovery-and-renaming>

???

And if you don't use garbage collection on the cache, those old targets will still be there even if newer ones exist too. If you are about to build a target, and it has the same versions of the same dependencies as some run-through from the past, you can activate data recovery to avoid running those commands again. This means if you invalidate targets, or you revert your code, or just try some ad hoc experiment and then come back, you can restore old targets and save time.

Unfortunately, this is not the default because it assumes you are always using the same packages and computing environment. This is where tools like `packrat` and `renv` and Docker come into play. They are great complements to what `drake` is doing, and they are fully compatible. drake does not solve every problem in reproducibility, but it tries to play nicely with tools that compensate for its relative weaknesses.

---

## Dependency-aware high-performance computing

- Just a little configuration...

```{r c11, eval = FALSE}
# template file with configuration
drake_hpc_template_file("slurm_clustermq.tmpl")

# Use SLURM resource manager with the template.
options(
  clustermq.scheduler = "slurm",
  clustermq.template = "slurm_clustermq.tmpl"
)

# make() is the basically the same.
make(plan, jobs = 2, parallelism = "clustermq")
```

???

And here's another feature that's been around for a long time: parallel computing. drake automatically decides which targets can run at the same time and which need to wait for dependencies. Whether you are running locally or on a cluster, it does not require too much configuring. How it works is you declare the number of workers, the parallel backend, and resource requirements if you're on a cluster..

---

## Dependency-aware high-performance computing

<iframe width="800" height="450" src="https://www.powtoon.com/embed/bUfSIaXjrw5/" frameborder="0"></iframe>

???

And drake takes care of the rest. It launches a bunch of workers and sends those workers to the targets as soon as they're ready to go.

---

## Resources

- Get [`drake`](https://github.com/ropensci/drake):

```{r c12, eval = FALSE}
install.packages("drake")
```

- Example code from these slides:

```{r c14, eval = FALSE}
drake::drake_example("customer-churn")
```

- Workshop materials:

```{r c13, eval = FALSE}
remotes::install_github("wlandau/learndrake")
```

???

Here's how to learn more about drake. Just as drake itself exists and operates entirely within R, so do many of its resources. You can install the package, get example code, and even take an entire 4-hour workshop without leaving R.

---

## Links

- Development repository: <https://github.com/ropensci/drake>
- Full user manual <https://ropenscilabs.github.io/drake-manual>
- Reference website: <https://docs.ropensci.org/drake>
- Hands-on workshop: <https://github.com/wlandau/learndrake>
- Code examples: <https://github.com/wlandau/drake-examples>
- Discuss at rOpenSci.org: <https://discuss.ropensci.org>

## rOpenSci use cases

- Use [`drake`](https://github.com/ropensci/drake)? Share your use case at <https://ropensci.org/usecases>.

<center>
<img src = "./images/ropensci.png" style="border: none; box-shadow: none; height: 150px">
</center>

???

Speaking of the workshop, you can take it right now. Just go to the GitHub page and click the "launch binder" badge. That'll take you to an RStudio Server instance on the cloud, and you can get started away. And by the way, Karthik's `holepunch` package made that super easy to set up.

If you use `drake` for a real project, please consider writing about it as an rOpenSci use case. It helps demonstrate the value of rOpenSci packages and grow the community.

---

## Thanks

<br>
<br>
<table style = "border: none">
<tr>
<td style = "padding-right: 125px">
<ul style>
<img src = "./images/edgar.jpg" style="border: none; box-shadow: none; height: 150px">
<li><a href = "https://github.com/edgararuiz">Edgar Ruiz</a></li>
<li><a href = "https://github.com/sol-eng/tensorflow-w-r/blob/master/workflow/tensorflow-drake.Rmd">example code</a></li>
</ul>
</td>
<td>
<ul>
<img src = "./images/matt.jpg" style="border: none; box-shadow: none; height: 150px">
<li><a href = "https://github.com/mdancho84">Matt Dancho</a></li>
<li><a href = "https://blogs.rstudio.com/tensorflow/posts/2018-01-11-keras-customer-churn/">blog post</a></li>
</ul>
</td>
</tr>
</table>

???

Today, I borrowed a deep learning example from Matt Dancho and Edgar Ruiz. Matt wrote a great RStudio blog post on deep learning in R, and Edgar engineered the solution to include `drake`.

---

## Thanks

<table style = "border: none">
<tr>
<td>
<br>
<ul>
<img src = "./images/ropensci.png" style="border: none; box-shadow: none; height: 150px">
<li><a href = "https://github.com/maelle">Maëlle Salmon</a></li>
<li><a href = "https://github.com/benmarwick">Ben Marwick</a></li>
<li><a href = "https://github.com/jules32">Julia Lowndes</a></li>
<li><a href = "https://github.com/gothub">Peter Slaughter</a></li>
<li><a href = "https://github.com/jennybc">Jenny Bryan</a></li>
<li><a href = "https://github.com/richfitz">Rich FitzJohn</a></li>
<li><a href = "https://github.com/stefaniebutland">Stefanie Butland</a></li>
</ul>
</td>
<td>
<ul>
<li><a href = "https://github.com/jarad">Jarad Niemi</a></li>
<li><a href = "https://github.com/krlmlr">Kirill Müller</a></li>
<li><a href = "https://github.com/HenrikBengtsson">Henrik Bengtsson</a></li>
<li><a href = "https://github.com/mschubert">Michael Schubert</a></li>
<li><a href = "https://github.com/kendonB">Kendon Bell</a></li>
<li><a href = "https://github.com/milesmcbain">Miles McBain</a></li>
<li><a href = "https://github.com/pat-s">Patrick Schratz</a></li>
<li><a href = "https://github.com/AlexAxthelm">Alex Axthelm</a></li>
<li><a href = "https://github.com/dapperjapper">Jasper Clarkberg</a></li>
<li><a href = "https://github.com/tiernanmartin">Tiernan Martin</a></li>
<li><a href = "https://github.com/BListyg">Ben Listyg</a></li>
<li><a href = "https://github.com/tjmahr">TJ Mahr</a></li>
<li><a href = "https://github.com/bpbond">Ben Bond-Lamberty</a></li>
<li><a href = "https://github.com/tmastny">Tim Mastny</a></li>
<li><a href = "https://github.com/billdenney">Bill Denney</a></li>
<li><a href = "https://github.com/aedobbyn">Amanda Dobbyn</a></li>
<li><a href = "https://github.com/dfalster">Daniel Falster</a></li>
<li><a href = "https://github.com/rkrug">Rainer Krug</a></li>
<li><a href = "https://github.com/bmchorse">Brianna McHorse</a></li>
<li><a href = "https://github.com/mrchypark">Chan-Yub Park</a></li>
</ul>
</td>
</tr>
</table>

???

For the development of drake itself, I have a lot of people to thank, many more than are on this list.

rOpenSci in particular is fantastic. It combines expertise and approachability, and it is the one of the friendliest and most welcoming communities in tech.

When they on-boarded `drake` into the suite of packages, it was technically a peer review process, but it felt more of a series of coaching sessions about documenting and explaining the package.

The rOpenSci folks also got the word out, and they put me in touch with collaborators all over the world and developers who I look up to. So thank you for your engagement and guidance and support.

---

## A riddle!

- From a math PhD oral exam:

> Give an example of a nontrivial function.

- Hint: the best answers do not even come from math or computing!

<center>
<img src = "./images/pythagoras.png" align="middle" style="border: none; box-shadow: none; height: 375px; text-align: center;">
<div style="font-size: 0.5em; text-align: center"><a href="https://publicdomainvectors.org/en/free-clipart/Pythagoras-tree/58775.html">https://publicdomainvectors.org/en/free-clipart/Pythagoras-tree/58775.html</a></div>
</center>

???

Let's end the talk portion with a riddle. For fun, try to think of an example of a nontrivial function.

This was a question from a math PhD oral exam, but you don't have to know any fancy math to think of something cool. Some students have said the Dirichlet function is nontrivial and the Weierstrass function is nontrivial. You can use a function to describe a fractal. But the math professor who posed this question would have thought these were all too trivial. You have to really believe that everything that happens is a function, and you have to make a function out something you might otherwise not think is possible.

```{r c15, include = FALSE}
tmp <- file.copy("R/plan.R", "R/plan2.R", overwrite = TRUE)
tmp <- file.copy("R/tmp.R", "R/plan.R", overwrite = TRUE)
unlink("R/tmp.R")
clean(destroy = TRUE)
unlink(".drake_history", recursive = TRUE, force = TRUE)
```
