---
title: "Solutions: build up a drake plan"
output: html_document
---

# Do first

1. Navigate to the `2-plans-solutions/` folder.
2. Open `2-plans-solutions.Rproj` as an RStudio project in a new R session. (Click on `2-plans-solutions.Rproj` in RStudio's file manager.)
3. Check that your working directory is correct. It should be `2-plans-solutions/`.

```{r}
basename(getwd()) # Should be "2-plans-solutions"
```

4. Run the setup code chunk below. (Click on the green arrow on the right.)

```{r, include = FALSE}
source("../config/options.R")
```

# About

To implement the customer churn prediction workflow, you will create a `drake` plan. A `drake` plan is a data frame with code. It has the *targets* you will build and the R *commands* to build those targets. Helpful references:

- The chapter on `drake` plans: <https://books.ropensci.org/drake/plans.html>
- The opening presentation of this workshop: <https://wlandau.github.io/learndrake>

# Dependencies

The commands in your `drake` plan will depend on the packages and functions introduced in `1-functions.Rmd`. Run code chunk below to load them now. (Click on the green arrow on the right.)

```{r message = FALSE}
source("R/packages.R")  # Load the packages.
source("R/functions.R") # Define our custom functions.
```

Take a quick moment to look at `packages.R` and `functions.R`. Open them in RStudio and see how they are organized.

# Start the plan

Your initial plan will

1. Read the customer churn data from a CSV file, and
2. Clean it up for the deep learning models.

Code for (1):

```{r, eval = FALSE}
# file_in() will tell drake to rebuild targets when the file changes.
churn_data <- split_data(file_in("../data/customer_churn.csv"))
```

Code for (2):

```{r, eval = FALSE}
churn_recipe <- prepare_recipe(churn_data)
```

Now, let's create a new `drake` plan with these two steps as *targets*:

```{r}
plan <- drake_plan(
  churn_data = split_data(file_in("../data/customer_churn.csv")),
  churn_recipe = prepare_recipe(churn_data)
)
```

The plan is a data frame with the targets and commands.

```{r, paged.print = FALSE}
plan
```

# Run the plan

When you first created the plan, you did not actually run the commands. To actually run the workflow, use `make()`.

```{r}
make(plan)
```

# Inspect the targets

Targets `churn_data` and `churn_recipe` now live in a special `.drake/` folder.

```{r}
list.files(all = TRUE)
```

The `.drake/` folder is a special kind of cache (<https://github.com/richfitz/storr>) and the files and subfolders are too complicated to work with manually.

```{r}
list.files(".drake/data")
```

To get the targets from the cache, use `drake` functions `loadd()` and `readd()`. Let's use them to check the data for potential problems.

```{r, paged.print = FALSE}
readd(churn_data)
```

```{r}
loadd(churn_recipe)
class(churn_recipe)
```

# Interlude: dependencies

Your workflow has several moving parts.

1. The two targets in the plan.
2. The CSV data you declared with `file_in()`.
3. The functions you loaded with `source("R/functions.R")`.

`drake` understands how these moving parts depend on each other. To see for yourself, visualize the graph.

```{r}
vis_drake_graph(plan)
```

Target `churn_data` depends on file `customer_churn.csv`, and target `churn_recipe` depends on function `prepare_recipe()` and target `churn_data`. 

```{r}
deps_target(churn_data, plan)
```

```{r}
deps_target(churn_recipe, plan)
```

`drake` learns all this through [static code analysis](http://adv-r.had.co.nz/Expressions.html#ast-funs), which detects the names of the symbols in the code without running the code just yet. The symbols you use determine the dependency relationships among the targets, which in turn determines execution order. That is why you can write targets in any order without breaking the plan.

```{r}
plan <- drake_plan(
  churn_recipe = prepare_recipe(churn_data), # previously last
  churn_data = split_data(file_in("../data/customer_churn.csv"))
)
```

We still get the same graph.

```{r}
vis_drake_graph(plan)
```

And our targets are still up to date.

```{r}
outdated(plan)
```

```{r}
make(plan)
```

If you *invalidate* the targets and run `make()` from scratch, `churn_recipe` still runs second. `make()` always uses *dependency relationships* to run the correct targets in the correct order.

```{r}
clean()
make(plan)
```

# Build up the plan

Let's add two model runs to the plan. One model has a relu activation function:

```{r, eval = FALSE}
# Do not run here.
run_relu <- test_model(act1 = "relu", churn_data, churn_recipe)
```

The other has a sigmoid activation function:

```{r, eval = FALSE}
# Do not run here.
run_sigmoid <- test_model(act1 = "sigmoid", churn_data, churn_recipe)
```

Your turn: add these commands to the new targets to the plan.

```{r}
plan <- drake_plan(
  churn_data = split_data(file_in("../data/customer_churn.csv")),
  churn_recipe = prepare_recipe(churn_data),
  run_relu = test_model(act1 = "relu", churn_data, churn_recipe),
  run_sigmoid = test_model(act1 = "sigmoid", churn_data, churn_recipe)
)
```

Visualize the graph to check the plan. `run_relu` and `run_sigmoid` should be new targets that depend on `churn_data`, `churn_recipe`, `test_model()`, and the custom functions called from `test_model()`.

```{r}
vis_drake_graph(plan)
```

`churn_data` and `churn_recipe` are up to date from last time, and `run_relu` and `run_sigmoid` are new.

```{r}
outdated(plan)
```

So `make()` skips the data and just runs the models. Run `make()` below and ignore any TensorFlow messages that pop up.

```{r}
make(plan) # slow
```

Those models took a long time to run, right? That is why `make()` skips them if they are up to date.

```{r}
outdated(plan)
```

```{r}
make(plan) # fast
```

Now is a good time to check your model runs. Each should be a data frame with the accuracy and features of a model.

```{r, paged.print = FALSE}
readd(run_relu)
```

```{r, paged.print = FALSE}
readd(run_sigmoid)
```

# Add another model

Now, let's add a third model with a different activation function. Use the `act1 = "softmax"` this time.

```{r}
plan <- drake_plan(
  churn_data = split_data(file_in("../data/customer_churn.csv")),
  churn_recipe = prepare_recipe(churn_data),
  run_relu = test_model(act1 = "relu", churn_data, churn_recipe),
  run_sigmoid = test_model(act1 = "sigmoid", churn_data, churn_recipe),
  run_softmax = test_model(act1 = "softmax", churn_data, churn_recipe)
)
```

The previous two runs should be up to date. Only `run_softmax` should be outdated.

```{r}
outdated(plan)
```

```{r}
vis_drake_graph(plan)
```

Run the softmax model.

```{r}
make(plan)
```

Did it come out right?

```{r, paged.print = FALSE}
readd(run_softmax)
```

# Pick the best model.

Define a new `best_run` target with following command:

```{r, eval = FALSE}
# Do not run here.
bind_rows(run_relu, run_sigmoid, run_softmax) %>%
  top_n(1, accuracy) %>%
  head(1)
```

Write the command in the plan below. Note: commands do not always need to be strict function calls. They can be arbitrary code chunks too, such as the one above.

```{r}
plan <- drake_plan(
  churn_data = split_data(file_in("../data/customer_churn.csv")),
  churn_recipe = prepare_recipe(churn_data),
  run_relu = test_model(act1 = "relu", churn_data, churn_recipe),
  run_sigmoid = test_model(act1 = "sigmoid", churn_data, churn_recipe),
  run_softmax = test_model(act1 = "softmax", churn_data, churn_recipe),
  best_run = bind_rows(run_relu, run_sigmoid, run_softmax) %>%
    top_n(1, accuracy) %>%
    head(1)
)
```

The next `make()` should just build `best_run` and be quick.

```{r}
make(plan)
```

`best_run` should contain one row with the accuracy and features of the best model.

```{r}
readd(best_run)
```

Now, let's train the best model and hold on to the trained model object. Below, we use `drake`'s `target()` function for extra customization because Keras models need the special `"keras"` storage format (<https://books.ropensci.org/drake/plans.html#special-data-formats-for-targets>). Below, write the command `train_best_model(best_run, churn_recipe)`.

```{r}
plan <- drake_plan(
  churn_data = split_data(file_in("../data/customer_churn.csv")),
  churn_recipe = prepare_recipe(churn_data),
  run_relu = test_model(act1 = "relu", churn_data, churn_recipe),
  run_sigmoid = test_model(act1 = "sigmoid", churn_data, churn_recipe),
  run_softmax = test_model(act1 = "softmax", churn_data, churn_recipe),
  best_run = bind_rows(run_relu, run_sigmoid, run_softmax) %>%
    top_n(1, accuracy) %>%
    head(1),
  best_model = target(
    train_best_model(best_run, churn_recipe),
    format = "keras"
  )
)
```

Train the model.

```{r}
make(plan)
```

The actual trained model is now in `drake`'s cache.

```{r}
readd(best_model)
```

# Changes

What if a function changes?

```{r}
define_model <- function(churn_recipe, units1, units2, act1, act2, act3) {
  input_shape <- ncol(
    juice(churn_recipe, all_predictors(), composition = "matrix")
  )
  keras_model_sequential() %>%
    layer_dense(
      units = units1,
      kernel_initializer = "uniform",
      activation = act1,
      input_shape = input_shape
    ) %>%
    layer_dropout(rate = 0.2) %>% # previously 0.1
    layer_dense(
      units = units2,
      kernel_initializer = "uniform",
      activation = act2
    ) %>%
    layer_dropout(rate = 0.1) %>%
    layer_dense(
      units = 1,
      kernel_initializer = "uniform",
      activation = act3
    )
}
```

`drake` automatically detects the change and rebuilds the affected targets.

```{r}
outdated(plan)
```

```{r}
vis_drake_graph(plan)
```

```{r}
make(plan)
```

What if we undo the change?

```{r}
define_model <- function(churn_recipe, units1, units2, act1, act2, act3) {
  input_shape <- ncol(
    juice(churn_recipe, all_predictors(), composition = "matrix")
  )
  keras_model_sequential() %>%
    layer_dense(
      units = units1,
      kernel_initializer = "uniform",
      activation = act1,
      input_shape = input_shape
    ) %>%
    layer_dropout(rate = 0.1) %>% # Changed back to 0.1.
    layer_dense(
      units = units2,
      kernel_initializer = "uniform",
      activation = act2
    ) %>%
    layer_dropout(rate = 0.1) %>%
    layer_dense(
      units = 1,
      kernel_initializer = "uniform",
      activation = act3
    )
}
```

You can tell `drake` to recover old targets instead of building new ones from scratch.

```{r}
make(plan, recover = TRUE)
```

Same story if you change a `file_in()` dataset (or change it back).

# Time savings and reproducibility

All your targets should be up to date now.

```{r}
outdated(plan)
```

```{r}
make(plan)
```

This is tangible evidence of reproducibility. It is tangible evidence that your output is in sync with the code and data you have right now. So if someone else were to run your code, they should get the same results as you. You can be confident without having to rerun everything from scratch. This is how `drake` saves time while increasing your ability to trust your research.

# Recap

Building up a plan is an gradual, iterative process:

1. Add or change some targets targets in the plan.
2. Check the plan with `outdated(plan)` and `vis_drake_graph(plan)`.
3. Run `make()` to build the new targets.
4. Check the output with `loadd()` and `readd()`.
5. Repeat.
