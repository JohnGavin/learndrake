---
title: "Building blocks of the customer churn case study"
output:
  html_document:
    df_print: paged
---

# Do first

1. Navigate to the `1-churn/` folder.
2. Open `1-churn.Rproj` as an RStudio project in a new R session. (Click on `1-churn.Rproj` in RStudio's file manager.)
3. Check that your working directory is correct. It should be `1-churn/`.

```{r}
basename(getwd()) # Should be "1-churn"
```

4. Run the setup chunk below.

```{r, include = FALSE}
source("../config/options.R")
```

# About

In this notebook, we will explore Keras models that predict customer behavior, and we will express our implementation in 3 custom functions. These functions are the essential building blocks of the `drake` workflows to come.

1. `prepare_recipe()`: preprocess the data.
2. `define_model()`: define a Keras model.
3. `train_model()`: train a model and summarize performance.

# Customer churn

Using the IBM Watson Telco Customer Churn dataset, we will train deep neural networks to classify customers. The goal is to predict who will cancel their subscription services such as internet and television. Cancellation, or "customer churn", is a problem that companies care about monitoring. For additional background on this example, please read <https://blogs.rstudio.com/tensorflow/posts/2018-01-11-keras-customer-churn>.

# Packages

Our functions will need the following R packages.

```{r, message = FALSE}
# Build and train deep neural nets.
# https://keras.rstudio.com/index.html
library(keras)

# Custom data preprocessing procedures.
# https://tidymodels.github.io/recipes/
library(recipes)

# Data resampling. We will use it to split the customer churn dataset
# into training and test sets for our deep learning models.
# https://tidymodels.github.io/rsample
library(rsample)

# Multiple packages that support clean code and tidy data.
# https://tidyverse.tidyverse.org/
library(tidyverse)
```

Check if TensorFlow is installed. The code below should display the TensorFlow version. Do not worry about other console messages.

```{r, message = FALSE}
library(tensorflow)
tf_version()
```

# Data

We have a local exerpt of the IBM Watson Telco Customer Churn dataset.

```{r}
raw_data <- read_csv("../data/customer_churn.csv", col_types = cols())
print(raw_data)
```

There are 21 variables (columns) on 7043 customers (rows).

```{r}
# View(raw_data)
glimpse(raw_data)
```

`Churn` is our response variable, and `customerID` identifies customers.

```{r}
raw_data %>%
  select(customerID, Churn) %>%
  print()
```

The rest of the variables are covariates.

- Subscriptions services: `PhoneService`, `MultipleLines`, `InternetService`, `OnlineSecurity`, `OnlineBackup`, `TechSupport`, `DeviceProtection`, `StreamingTV`, and `StreamingMovies`.
- Account information: `Contract`, `PaymentMethod`, `PaperlessBilling`, `tenure`, `MonthlyCharges`, and `TotalCharges`.
- Demographic information: `gender`, `SeniorCitizen`, `Partner`, and `Dependents`.

## Train/test split

For machine learning, we need to split the customers into a training dataset and a testing dataset.

```{r}
churn_data <- initial_split(raw_data, prop = 0.3) # from the rsample package
```

Our training set has 2113 customers, and our testing set has 4930.

```{r}
print(churn_data)
```

Functions from [`rsample`](https://tidymodels.github.io/rsample) can recover the training and testing sets.

```{r}
training(churn_data)
```

```{r}
testing(churn_data)
```

# Functions

## `prepare_recipe()`

`prepare_recipe()` encapsulates everything we need to do to get the data ready for the models. It accepts a data frame with train/test splits from `rsample` and returns a recipe object generated by the `recipes` package.

```{r}
prepare_recipe <- function(churn_data) {
  churn_data %>%
    # Just preprocess the training data.
    training() %>%
    # Start defining a new recipe.
    recipe(Churn ~ .) %>%
    # Remove the customerID variable from the data.
    step_rm(customerID) %>%
    # Remove missing values.
    step_naomit(all_outcomes(), all_predictors()) %>%
    # Partition the tenure variable into 6 bins.
    step_discretize(tenure, options = list(cuts = 6)) %>%
    # Take the log of TotalCharges to strengthen the association with Churn.
    step_log(TotalCharges) %>%
    # Encode the Churn variable as a 0-1 indicator variable.
    step_mutate(Churn = ifelse(Churn == "Yes", 1, 0)) %>%
    # Encode each categorical variable as a collection of 0-1 indicators.
    step_dummy(all_nominal(), -all_outcomes()) %>%
    # Center all covariates.
    step_center(all_predictors(), -all_outcomes()) %>%
    # Scale all covariates.
    step_scale(all_predictors(), -all_outcomes()) %>%
    # Run the recipe on the data.
    prep()
}
```

Let's try out the function to make sure it works.

```{r}
churn_recipe <- prepare_recipe(churn_data)
print(churn_recipe)
```

Later on, we will need to retrieve the preprocessed training data with `juice()`.

```{r}
juice(churn_recipe, all_outcomes())
```

```{r}
juice(churn_recipe, all_predictors())
```

Keras will expect our predictors to be in matrix form.

```{r}
juice(churn_recipe, all_predictors(), composition = "matrix")[1:6, 1:4]
```

To evaluate model performance, we will need `bake()` to run the recipe on the testing data.

```{r}
bake(churn_recipe, testing(churn_data))
```

## `define_model()`

The `define_model()` function encapsulates our Keras model specification in a convenient, reusable, easy-to-read form. The function accepts a prepped recipe and some hyperparameters and returns a model definition.

```{r}
define_model <- function(churn_recipe, units1, units2, act1, act2, act3) {
  input_shape <- ncol(
    juice(churn_recipe, all_predictors(), composition = "matrix")
  )
  keras_model_sequential() %>%
    layer_dense(
      units = units1,
      kernel_initializer = "uniform",
      activation = act1,
      input_shape = input_shape
    ) %>%
    layer_dropout(rate = 0.1) %>%
    layer_dense(
      units = units2,
      kernel_initializer = "uniform",
      activation = act2
    ) %>%
    layer_dropout(rate = 0.1) %>%
    layer_dense(
      units = 1,
      kernel_initializer = "uniform",
      activation = act3
    )
}
```

Let's check if it returns a valid model.

```{r}
define_model(churn_recipe, 16, 16, "relu", "relu", "sigmoid") %>%
  print()
```

## `train_model()`

We write a whole new function that accepts a recipe and hyperparameters and returns a tidy data frame to summarize the hyperparameters and validation accuracy. Below, we use `define_model()` as custom shorthand to make `train_model()` easier to read.

```{r}
train_model <- function(
  churn_recipe,
  units1 = 16,
  units2 = 16,
  act1 = "relu",
  act2 = "relu",
  act3 = "sigmoid"
) {
  model <- define_model(
    churn_recipe = churn_recipe,
    units1 = units1,
    units2 = units2,
    act1 = act1,
    act2 = act2,
    act3 = act3
  )
  compile(
    model,
    optimizer = "adam",
    loss = "binary_crossentropy",
    metrics = c("accuracy")
  )
  x_train_tbl <- juice(
    churn_recipe,
    all_predictors(),
    composition = "matrix"
  )
  y_train_vec <- juice(churn_recipe, all_outcomes()) %>%
    pull()
  history <- fit(
    object = model,
    x = x_train_tbl,
    y = y_train_vec,
    batch_size = 32,
    epochs = 32,
    validation_split = 0.3,
    verbose = 0
  )
  history %>%
    as_tibble() %>%
    filter(epoch == max(epoch) & metric == "acc" & data == "validation") %>%
    select(value) %>%
    rename(accuracy = value) %>%
    mutate(
      units1 = units1,
      units2 = units2,
      act1 = act1,
      act2 = act2,
      act3 = act3
    )
}
```

Let's try out `train_model()` to make sure it works. It takes a long time to run, which is exactly why we will use `drake` to manage the reproducible workflow later on.

If you get messages like "your CPU supports instructions that this TensorFlow binary was not compiled to use", do not worry about it. It is totally normal and not an error.

```{r, output = FALSE}
train_model(churn_recipe, units1 = 16, units2 = 16)
```
