---
title: "Set up a drake workflow"
output: html_notebook
---

# Do first

1. Navigate to the `2-setup/` folder.
2. Open `2-setup.Rproj` as an RStudio project in a new R session. (Click on `2-setup.Rproj` in RStudio's file manager.)
3. Check that your working directory is correct. It should be `2-setup/`.

```{r}
basename(getwd()) # Should be "2-setup"
```

4. Run the setup chunk below.

```{r, include = FALSE}
source("../config/options.R")
```

# About

After working through `1-churn.Rmd`, we have the building blocks (i.e. functions) of our data analysis project. Now, let's put the pieces together in a `drake`-powered reproducible workflow.

You may find these references on `drake` helpful (but hopefully not necessary for this notebook).

- GitHub page: <https://github.com/ropensci/drake>
- Reference website: <https://docs.ropensci.org/drake/>
- User manual: <https://ropenscilabs.github.io/drake-manual/>
- Example code: <https://github.com/wlandau/drake-examples>
- `drakeplanner` app: <https://github.com/wlandau/drakeplanner>
- Existing presentations: <https://ropenscilabs.github.io/drake-manual/index.html#presentations>
- This workshop: <https://github.com/wlandau/learndrake>

# Dependencies

To set up a `drake` workflow for the customer churn case study, we first load our packages and functions into the current R session.

```{r message = FALSE}
source("R/packages.R")  # Load the packages.
source("R/functions.R") # Define our custom functions.
```

Open up `packages.R` and `functions.R` scripts and take a look at how they are organized.

# Plan

Now, it is time to plan the actual data analysis. If you were to write an R script, this is what the workflow would look like.

```{r, eval = FALSE}
# Get the data.
data <- read_csv(
  file_in("../data/customer_churn.csv"),
  col_types = cols()
) %>%
  initial_split(prop = 0.3)

# Create the recipe.
rec <- prepare_recipe(data)

# Train the model.
model <- train_model(rec)

# Compare to testing data.
conf <- confusion_matrix(data, rec, model)

# Compute performance metrics.
metrics <- compare_models(conf)
```

But for `drake`-powered automation and reproducibility, we use a special data frame called a "`drake` plan" (<https://ropenscilabs.github.io/drake-manual/plans.html>).

Now it is your turn: fill out the `drake` plan below using the code from the previous chunk. The "targets" (`data`, `rec`, etc.) can be in any order.

```{r}
plan <- drake_plan(
	# Create the recipe.
  rec = ,

  # Train the model.
  model = target(
  	, # Write a call to train_model() to the left of the comma.
    format = "keras" # Tells drake to save the model as an HDF5 file.
  ),

  # Compare to testing data.
  conf = ,

  # Compute performance metrics.
  metrics = ,

  # Get the data. Filled in for you already.
  data = read_csv(
    file_in("../data/customer_churn.csv"),
    col_types = cols()
  ) %>%
    initial_split(prop = 0.3)
)
```

The plan is just a fancy data frame. Each command is a step in the workflow, and each target will store the return value of a command. 

```{r, paged.print = FALSE, warning = FALSE}
plan
```

# Check the plan.

Did you write the plan correctly? Let's find out with a dependency graph. The graph is interactive. Zoom and drag for a better view.

```{r, message = FALSE}
config <- drake_config(plan)
vis_drake_graph(config, hover = TRUE)
```

If you wrote the plan properly, your graph should look something like this.

```{r}
library(visNetwork)
readRDS("img/graph.rds")
```

# Do the work.

Call [`make()`](https://docs.ropensci.org/drake/reference/make.html) to actually run the workflow. `make()` builds the targets from left to right in the dependency graph. The output is stored in a hidden `.drake/` cache.

```{r}
make(plan)
```

Try `make()` again. What happens?

```{r}
make(plan)
```

The graph displays the status of targets.

```{r}
vis_drake_graph(config)
```

Let's load some targets from on-disk storage.

```{r}
loadd(data)
training(data) %>%
  select(Churn, customerID, TotalCharges) %>%
  head(n = 6)
```

```{r}
readd(metrics) # see also loadd()
```

# History

Using `drake_history()`, you can see which models you ran, when you ran them, how long they took, and which settings you tried.

```{r, paged.print = FALSE}
history <- drake_history()
history
```

And as long as you did not run `clean(garbage_collection = TRUE)`, you can get the old data back. Let's find the oldest run of the model.

```{r}
hash <- history %>%
  filter(target == "model") %>%
  pull(hash) %>%
  head(n = 1)
drake_cache()$get_value(hash)
```

# Data recovery (experimental)

Repeated `make()`s, as well as `clean()`, can alter the status of current targets. 

```{r}
clean()
```

But again, unless you call `clean(garbage_collection = TRUE)`, the actual data is still there! `make(plan, recover = TRUE)` can reinsert an old target back into the pipeline, as long as there exists an old run with the same command, function dependencies, etc. that it has right now.

```{r}
make(plan, recover = TRUE)
```

In fact, you can even rename a target this way: <https://ropenscilabs.github.io/drake-manual/walkthrough.html#automated-recovery-and-renaming>.
